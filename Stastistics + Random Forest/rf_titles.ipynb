{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-04T08:00:56.622650Z",
     "start_time": "2025-02-04T08:00:56.619984Z"
    }
   },
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "source": [
    "# De aici: https://en.wiktionary.org/wiki/Category:Romanian_prefixes\n",
    "romanian_prefixes = [\n",
    "    # A\n",
    "    \"agro\", \"alt\", \"ante\", \"anti\", \"aorto\", \"arhi\", \"astro\",\n",
    "\n",
    "    # B\n",
    "    \"balano\",\n",
    "\n",
    "    # C\n",
    "    \"cardio\", \"carpo\", \"cosmo\",\n",
    "\n",
    "    # D\n",
    "    \"demono\", \"des\", \"dez\",\n",
    "\n",
    "    # F\n",
    "    \"franco\",\n",
    "\n",
    "    # G\n",
    "    \"gastro\", \"germano\", \"greco\",\n",
    "\n",
    "    # H\n",
    "    \"hecto\", \"hiper\",\n",
    "\n",
    "    # I\n",
    "    \"în\",\n",
    "\n",
    "    # K\n",
    "    \"kilo\",\n",
    "\n",
    "    # L\n",
    "    \"lexico\",\n",
    "\n",
    "    # M\n",
    "    \"mili\", \"muzico\",\n",
    "\n",
    "    # N\n",
    "    \"nano\", \"ne\",\n",
    "\n",
    "    # O\n",
    "    \"ori\", \"ornito\",\n",
    "\n",
    "    # P\n",
    "    \"pneumo\", \"pre\", \"prea\", \"proto\", \"pseudo\", \"psiho\",\n",
    "\n",
    "    # R\n",
    "    \"răs\", \"re\", \"rino\", \"ruso\",\n",
    "\n",
    "    # S\n",
    "    \"stră\", \"sub\",\n",
    "\n",
    "    # T\n",
    "    \"tehno\", \"teo\", \"termo\",\n",
    "\n",
    "    # V\n",
    "    \"vice\"\n",
    "]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T08:00:56.634510Z",
     "start_time": "2025-02-04T08:00:56.625026Z"
    }
   },
   "id": "48c4959a309f48ab",
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "source": [
    "def replace_i_prefix(word, prefixes):\n",
    "  for prefix in prefixes:\n",
    "    try:\n",
    "      if word.lower().startswith(prefix) and len(word) > len(prefix) and word[len(prefix):][0] in [\"î\", \"Î\"]:\n",
    "        first_letter = word[len(prefix):][0]\n",
    "        first_letter = \"i\" if first_letter == \"î\" else (\"I\" if first_letter == \"Î\" else first_letter)\n",
    "        word = prefix + first_letter + word[len(prefix) + 1:]\n",
    "\n",
    "    except:\n",
    "      print(word)\n",
    "\n",
    "  word = word.replace(\"î\", \"a\").replace(\"Î\", \"A\")\n",
    "\n",
    "  return word\n",
    "\n",
    "def no_diacritics(text, prefixes):\n",
    "\n",
    "  text = replace_i_prefix(text, prefixes)\n",
    "\n",
    "\n",
    "  text = text.replace(\"â\", \"i\")\n",
    "  text = text.replace(\"Â\", \"I\")\n",
    "  text = text.replace(\"ș\", \"s\")\n",
    "  text = text.replace(\"ş\", \"s\")\n",
    "  text = text.replace(\"Ș\", \"S\")\n",
    "  text = text.replace(\"Ş\", \"S\")\n",
    "  text = text.replace(\"ț\", \"t\")\n",
    "  text = text.replace(\"ţ\", \"t\")\n",
    "  text = text.replace(\"Ț\", \"T\")\n",
    "  text = text.replace(\"Ţ\", \"T\")\n",
    "\n",
    "  # If î is the first letter of the word, replace it with i\n",
    "  if text.startswith(\"î\"):\n",
    "    text = text.replace(\"î\", \"i\")\n",
    "  if text.startswith(\"Î\"):\n",
    "    text = text.replace(\"Î\", \"I\")\n",
    "  # If the last letter of the word is î, replace it with i\n",
    "  if text.endswith(\"î\"):\n",
    "    text = text.replace(\"î\", \"i\")\n",
    "  if text.endswith(\"Î\"):\n",
    "    text = text.replace(\"Î\", \"I\")\n",
    "  # Else replace î with a\n",
    "  if \"î\" in text:\n",
    "    text = text.replace(\"î\", \"a\")\n",
    "  # text = text.replace(\"î\", \"i\")\n",
    "  # text = text.replace(\"Î\", \"I\")\n",
    "  text = text.replace(\"ă\", \"a\")\n",
    "  text = text.replace(\"Ă\", \"A\")\n",
    "\n",
    "  return text\n",
    "\n",
    "\n",
    "# for key in moldavian_texts:\n",
    "#     for i in range(len(moldavian_texts[key])):\n",
    "#         moldavian_texts[key][i] = no_diacritics(moldavian_texts[key][i], romanian_prefixes)\n",
    "\n",
    "# for key in romanian_texts:\n",
    "#     for i in range(len(romanian_texts[key])):\n",
    "#         romanian_texts[key][i] = no_diacritics(romanian_texts[key][i], romanian_prefixes)\n",
    "\n",
    "# print(moldavian_texts[\"Sport\"][0])\n",
    "# print(romanian_texts['Stiri'][12])\n",
    "\n",
    "print(no_diacritics(\"cîțiva\", romanian_prefixes))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T08:00:56.675218Z",
     "start_time": "2025-02-04T08:00:56.670467Z"
    }
   },
   "id": "892a96c01a32732f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cativa\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "source": [
    "romanian=[\n",
    "    \"a\", \"abia\", \"acea\", \"aceasta\", \"această\", \"aceea\", \"aceeasi\", \"acei\",\n",
    "    \"aceia\", \"acel\", \"acela\", \"acelasi\", \"acele\", \"acelea\", \"acest\", \"acesta\",\n",
    "    \"aceste\", \"acestea\", \"acestei\", \"acestia\", \"acestui\", \"aceşti\", \"aceştia\",\n",
    "    \"acești\", \"aceștia\", \"acolo\", \"acord\", \"acum\", \"adica\", \"ai\", \"aia\",\n",
    "    \"aibă\", \"aici\", \"aiurea\", \"al\", \"ala\", \"alaturi\", \"ale\", \"alea\", \"alt\",\n",
    "    \"alta\", \"altceva\", \"altcineva\", \"alte\", \"altfel\", \"alti\", \"altii\", \"altul\",\n",
    "    \"alături\", \"am\", \"anume\", \"apoi\", \"ar\", \"are\", \"as\", \"asa\", \"asemenea\",\n",
    "    \"asta\", \"astazi\", \"astea\", \"astfel\", \"astăzi\", \"asupra\", \"atare\", \"atat\",\n",
    "    \"atata\", \"atatea\", \"atatia\", \"ati\", \"atit\", \"atita\", \"atitea\", \"atitia\",\n",
    "    \"atunci\", \"au\", \"avea\", \"avem\", \"aveţi\", \"aveți\", \"avut\", \"azi\", \"aş\",\n",
    "    \"aşadar\", \"aţi\", \"aș\", \"așadar\", \"ați\", \"b\", \"ba\", \"bine\", \"bucur\", \"bună\",\n",
    "    \"c\", \"ca\", \"cam\", \"cand\", \"capat\", \"care\", \"careia\", \"carora\", \"caruia\",\n",
    "    \"cat\", \"catre\", \"caut\", \"ce\", \"cea\", \"ceea\", \"cei\", \"ceilalti\", \"cel\",\n",
    "    \"cele\", \"celor\", \"ceva\", \"chiar\", \"ci\", \"cinci\", \"cind\", \"cine\", \"cineva\",\n",
    "    \"cit\", \"cita\", \"cite\", \"citeva\", \"citi\", \"câțiva\", \"conform\", \"contra\",\n",
    "    \"cu\", \"cui\", \"cum\", \"cumva\", \"curând\", \"curînd\", \"când\", \"cât\", \"câte\",\n",
    "    \"câtva\", \"câţi\", \"câți\", \"cînd\", \"cît\", \"cîte\", \"cîtva\", \"cîţi\", \"cîți\",\n",
    "    \"că\", \"căci\", \"cărei\", \"căror\", \"cărui\", \"către\", \"d\", \"da\", \"daca\",\n",
    "    \"dacă\", \"dar\", \"dat\", \"datorită\", \"dată\", \"dau\", \"de\", \"deasupra\", \"deci\",\n",
    "    \"decit\", \"degraba\", \"deja\", \"deoarece\", \"departe\", \"desi\", \"despre\",\n",
    "    \"deşi\", \"deși\", \"din\", \"dinaintea\", \"dintr\", \"dintr-\", \"dintre\", \"doar\",\n",
    "    \"doi\", \"doilea\", \"două\", \"drept\", \"dupa\", \"după\", \"dă\", \"e\", \"ea\", \"ei\",\n",
    "    \"el\", \"ele\", \"era\", \"eram\", \"este\", \"eu\", \"exact\", \"eşti\", \"ești\", \"f\",\n",
    "    \"face\", \"fara\", \"fata\", \"fel\", \"fi\", \"fie\", \"fiecare\", \"fii\", \"fim\", \"fiu\",\n",
    "    \"fiţi\", \"fiți\", \"foarte\", \"fost\", \"frumos\", \"fără\", \"g\", \"geaba\", \"graţie\",\n",
    "    \"grație\", \"h\", \"halbă\", \"i\", \"ia\", \"iar\", \"ieri\", \"ii\", \"il\", \"imi\", \"in\",\n",
    "    \"inainte\", \"inapoi\", \"inca\", \"incit\", \"insa\", \"intr\", \"intre\", \"isi\",\n",
    "    \"iti\", \"j\", \"k\", \"l\", \"la\", \"le\", \"li\", \"lor\", \"lui\", \"lângă\", \"lîngă\",\n",
    "    \"m\", \"ma\", \"mai\", \"mare\", \"mea\", \"mei\", \"mele\", \"mereu\", \"meu\", \"mi\",\n",
    "    \"mie\", \"mine\", \"mod\", \"mult\", \"multa\", \"multe\", \"multi\", \"multă\", \"mulţi\",\n",
    "    \"mulţumesc\", \"mulți\", \"mulțumesc\", \"mâine\", \"mîine\", \"mă\", \"n\", \"ne\",\n",
    "    \"nevoie\", \"ni\", \"nici\", \"niciodata\", \"nicăieri\", \"nimeni\", \"nimeri\",\n",
    "    \"nimic\", \"niste\", \"nişte\", \"niște\", \"noastre\", \"noastră\", \"noi\", \"noroc\",\n",
    "    \"nostri\", \"nostru\", \"nou\", \"noua\", \"nouă\", \"noştri\", \"noștri\", \"nu\",\n",
    "    \"numai\", \"o\", \"opt\", \"or\", \"ori\", \"oricare\", \"orice\", \"oricine\", \"oricum\",\n",
    "    \"oricând\", \"oricât\", \"oricînd\", \"oricît\", \"oriunde\", \"p\", \"pai\", \"parca\",\n",
    "    \"patra\", \"patru\", \"patrulea\", \"pe\", \"pentru\", \"peste\", \"pic\", \"pina\",\n",
    "    \"plus\", \"poate\", \"pot\", \"prea\", \"prima\", \"primul\", \"prin\", \"printr-\",\n",
    "    \"putini\", \"puţin\", \"puţina\", \"puţină\", \"puțin\", \"puțina\", \"puțină\", \"până\",\n",
    "    \"pînă\", \"r\", \"rog\", \"s\", \"sa\", \"sa-mi\", \"sa-ti\", \"sai\", \"sale\", \"sau\",\n",
    "    \"se\", \"si\", \"sint\", \"sintem\", \"spate\", \"spre\", \"sub\", \"sunt\", \"suntem\",\n",
    "    \"sunteţi\", \"sunteți\", \"sus\", \"sută\", \"sînt\", \"sîntem\", \"sînteţi\",\n",
    "    \"sînteți\", \"să\", \"săi\", \"său\", \"t\", \"ta\", \"tale\", \"te\", \"ti\", \"timp\",\n",
    "    \"tine\", \"toata\", \"toate\", \"toată\", \"tocmai\", \"tot\", \"toti\", \"totul\",\n",
    "    \"totusi\", \"totuşi\", \"totuși\", \"toţi\", \"toți\", \"trei\", \"treia\", \"treilea\",\n",
    "    \"tu\", \"tuturor\", \"tăi\", \"tău\", \"u\", \"ul\", \"ului\", \"un\", \"una\", \"unde\",\n",
    "    \"undeva\", \"unei\", \"uneia\", \"unele\", \"uneori\", \"unii\", \"unor\", \"unora\",\n",
    "    \"unu\", \"unui\", \"unuia\", \"unul\", \"v\", \"va\", \"vi\", \"voastre\", \"voastră\",\n",
    "    \"voi\", \"vom\", \"vor\", \"vostru\", \"vouă\", \"voştri\", \"voștri\", \"vreme\", \"vreo\",\n",
    "    \"vreun\", \"vă\", \"x\", \"z\", \"zece\", \"zero\", \"zi\", \"zice\", \"îi\", \"îl\", \"îmi\",\n",
    "    \"împotriva\", \"în\", \"înainte\", \"înaintea\", \"încotro\", \"încât\", \"încît\",\n",
    "    \"între\", \"întrucât\", \"întrucît\", \"îţi\", \"îți\", \"ăla\", \"ălea\", \"ăsta\",\n",
    "    \"ăstea\", \"ăştia\", \"ăștia\", \"şapte\", \"şase\", \"şi\", \"ştiu\", \"ţi\", \"ţie\",\n",
    "    \"șapte\", \"șase\", \"și\", \"știu\", \"ți\", \"ție\"\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T08:00:56.727656Z",
     "start_time": "2025-02-04T08:00:56.719129Z"
    }
   },
   "id": "6f424384dbf01e6b",
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "source": [
    "# # Get all the words from the stop words list and apply the same transformation\n",
    "stop_words = romanian\n",
    "for i in range(len(stop_words)):\n",
    "    stop_words[i] = no_diacritics(stop_words[i], romanian_prefixes)\n",
    "\n",
    "stop_words = list(set(stop_words))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T08:00:56.776720Z",
     "start_time": "2025-02-04T08:00:56.770437Z"
    }
   },
   "id": "a152024aa25a71cb",
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_label_distribution(dataset_folder):\n",
    "    json_files = [f for f in os.listdir(dataset_folder) if f.endswith('.json')]\n",
    "    \n",
    "    dataframes = []\n",
    "    \n",
    "    for filename in json_files:\n",
    "        filepath = os.path.join(dataset_folder, filename)\n",
    "        \n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if 200 <= len(data) <= 1000:\n",
    "            df = pd.DataFrame(data)\n",
    "            dataframes.append(df)\n",
    "        elif len(data) > 1000:\n",
    "            df = pd.DataFrame(data).sample(n=1000, random_state=42)\n",
    "            dataframes.append(df)\n",
    "    \n",
    "    # Combine dataframes\n",
    "    if dataframes:\n",
    "        final_dataframe = pd.concat(dataframes, ignore_index=True)\n",
    "        \n",
    "        # Count label distribution\n",
    "        label_counts = final_dataframe['label'].value_counts()\n",
    "        \n",
    "        print(\"Label Distribution:\")\n",
    "        for label, count in label_counts.items():\n",
    "            print(f\"{label}: {count}\")\n",
    "        \n",
    "        return final_dataframe\n",
    "    else:\n",
    "        print(\"No files met the criteria.\")\n",
    "        return None\n",
    "\n",
    "# Run the analysis\n",
    "dataset_folder = 'Dataset'\n",
    "data = analyze_label_distribution(dataset_folder)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T08:00:57.447096Z",
     "start_time": "2025-02-04T08:00:56.819836Z"
    }
   },
   "id": "4f053a001f19b23e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Distribution:\n",
      "Banat: 1000\n",
      "Serbia: 1000\n",
      "Moldova: 1000\n",
      "Muntenia: 1000\n",
      "Ungheni: 1000\n",
      "Ardeal: 1000\n",
      "Oltenia: 1000\n",
      "Ucraina: 1000\n",
      "Dobrogea: 965\n",
      "Balti: 948\n",
      "Sangerei: 775\n",
      "Spania: 723\n",
      "Maramures: 656\n",
      "Canada_EN: 641\n",
      "Crisana: 579\n",
      "Orhei: 512\n",
      "Calarasi: 511\n",
      "Criuleni: 509\n",
      "Ialoveni: 504\n",
      "Cahul: 504\n",
      "Soroca: 504\n",
      "Germania: 500\n",
      "UK: 499\n",
      "Bucovina: 428\n",
      "Causeni: 321\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "source": [
    "def get_features_1(train_text, validation_text, num_features):\n",
    "    tfidf = TfidfVectorizer(max_features=num_features)\n",
    "\n",
    "    train_text_tfidf = tfidf.fit_transform(train_text)\n",
    "\n",
    "    validation_text_tfidf = tfidf.transform(validation_text)\n",
    "\n",
    "    return train_text_tfidf.toarray(), validation_text_tfidf.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T08:00:57.456787Z",
     "start_time": "2025-02-04T08:00:57.454975Z"
    }
   },
   "id": "5f9f92824568c261",
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "source": [
    "nlp = spacy.load(\"ro_core_news_sm\")\n",
    "\n",
    "def preprocess(text):\n",
    "    text = no_diacritics(text, romanian_prefixes)\n",
    "    text = ' '.join(word for word in text.split() if word.lower() not wein stop_words)\n",
    "    doc = nlp(text)\n",
    "    text = ' '.join(token.lemma_ for token in doc if not token.is_stop)\n",
    "    \n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T08:00:57.500267Z",
     "start_time": "2025-02-04T08:00:57.497742Z"
    }
   },
   "id": "ee90ddb829549a79",
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1189633427.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[42], line 5\u001B[0;36m\u001B[0m\n\u001B[0;31m    text = ' '.join(word for word in text.split() if word.lower() not wein stop_words)\u001B[0m\n\u001B[0m                                                                      ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "source": [
    "data = data[data['title'].str.strip() != ''].dropna(subset=['title'])\n",
    "\n",
    "titles = data['title']\n",
    "labels = data['label']\n",
    "\n",
    "train_titles, validation_titles, train_labels, validation_labels = train_test_split(titles, \n",
    "                                                                                  labels, test_size=0.2, random_state=42)\n",
    "\n",
    "train_titles.apply(preprocess)\n",
    "validation_titles.apply(preprocess)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T08:00:57.542535788Z",
     "start_time": "2025-02-04T07:40:49.818687Z"
    }
   },
   "id": "7abfb665b4c69d9b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3270     an\n",
       "4167       \n",
       "2959       \n",
       "6815       \n",
       "7355       \n",
       "         ..\n",
       "19048      \n",
       "13338      \n",
       "14678      \n",
       "2448     an\n",
       "1888     an\n",
       "Name: title, Length: 3815, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_titles = train_titles[train_titles.str.strip() != '']\n",
    "train_titles = train_titles[train_titles.notna()]\n",
    "validation_titles = validation_titles[validation_titles.str.strip() != '']\n",
    "validation_titles = validation_titles[validation_titles.notna()]\n",
    "\n",
    "train_labels = train_labels[train_titles.index]\n",
    "train_labels = train_labels[train_labels.notna()]\n",
    "validation_labels = validation_labels[validation_titles.index]\n",
    "validation_labels = validation_labels[validation_labels.notna()]\n",
    "\n",
    "final_train_indices = train_titles.index.intersection(train_labels.index)\n",
    "final_val_indices = validation_titles.index.intersection(validation_labels.index)\n",
    "\n",
    "train_titles = train_titles[final_train_indices]\n",
    "train_labels = train_labels[final_train_indices]\n",
    "validation_titles = validation_titles[final_val_indices]\n",
    "validation_labels = validation_labels[final_val_indices]\n",
    "\n",
    "print(\"\\nFinal dataset sizes after cleaning:\")\n",
    "print(f\"Final training samples: {len(train_titles)}\")\n",
    "print(f\"Final validation samples: {len(validation_titles)}\")"
   ],
   "id": "f81f0e5f60009802"
  },
  {
   "cell_type": "code",
   "source": [
    "f = open(\"rf-titles-tfidf.txt\", \"w\")\n",
    "\n",
    "for num_features in [500, 1000, 1500, 2000, 2500]:\n",
    "    for num_estimators in [100, 150, 200]:\n",
    "        print(\"=\" * 50)\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "        print(f\"Testing for num_features = {num_features}, num_estimators = {num_estimators}\")\n",
    "        f.write(f\"Testing for num_features = {num_features}, num_estimators = {num_estimators}\\n\")\n",
    "\n",
    "        print(\"Creating text features...\")\n",
    "        f.write(\"Creating text features...\\n\")\n",
    "\n",
    "        train_titles_featurized, validation_titles_featurized = get_features_1(train_titles, validation_titles, num_features)\n",
    "\n",
    "        print(\"Training Random Forest...\")\n",
    "        f.write(\"Training Random Forest...\\n\")\n",
    "        random_forest = RandomForestClassifier(\n",
    "            n_estimators=num_estimators,\n",
    "            random_state=42)\n",
    "        random_forest.fit(train_titles_featurized, train_labels)\n",
    "\n",
    "        print(\"Getting predictions...\")\n",
    "        f.write(\"Getting predictions...\\n\")\n",
    "        predictions = random_forest.predict(validation_titles_featurized)\n",
    "        \n",
    "        print(\"Computing accuracy...\")\n",
    "        f.write(\"Computing accuracy...\")\n",
    "        accuracy = accuracy_score(validation_labels, predictions)\n",
    "\n",
    "        print(\"Computing f1...\")\n",
    "        f.write(\"Computing f1...\\n\")\n",
    "        f1 = f1_score(validation_labels, predictions, average='weighted')\n",
    "\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        f.write(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "        print(f\"F1: {f1}\")\n",
    "        f.write(f\"F1: {f1}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T08:00:57.547238444Z",
     "start_time": "2025-02-04T07:42:25.954851Z"
    }
   },
   "id": "3a7cc62f2cd35f2c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Testing for num_features = 500, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 21\u001B[0m\n\u001B[1;32m     17\u001B[0m f\u001B[38;5;241m.\u001B[39mwrite(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining Random Forest...\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     18\u001B[0m random_forest \u001B[38;5;241m=\u001B[39m RandomForestClassifier(\n\u001B[1;32m     19\u001B[0m     n_estimators\u001B[38;5;241m=\u001B[39mnum_estimators,\n\u001B[1;32m     20\u001B[0m     random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[0;32m---> 21\u001B[0m \u001B[43mrandom_forest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_titles_featurized\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_labels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGetting predictions...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     24\u001B[0m f\u001B[38;5;241m.\u001B[39mwrite(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGetting predictions...\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Desktop/Proiecte/venvs/llm_and_cognition_stats/lib/python3.12/site-packages/sklearn/base.py:1389\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1382\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1384\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1385\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1386\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1387\u001B[0m     )\n\u001B[1;32m   1388\u001B[0m ):\n\u001B[0;32m-> 1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Proiecte/venvs/llm_and_cognition_stats/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:360\u001B[0m, in \u001B[0;36mBaseForest.fit\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m    357\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m issparse(y):\n\u001B[1;32m    358\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msparse multilabel-indicator for y is not supported.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 360\u001B[0m X, y \u001B[38;5;241m=\u001B[39m \u001B[43mvalidate_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    361\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    362\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    363\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    364\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmulti_output\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    365\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsc\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    366\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mDTYPE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    367\u001B[0m \u001B[43m    \u001B[49m\u001B[43mensure_all_finite\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    368\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    369\u001B[0m \u001B[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001B[39;00m\n\u001B[1;32m    370\u001B[0m \u001B[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001B[39;00m\n\u001B[1;32m    371\u001B[0m \u001B[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001B[39;00m\n\u001B[1;32m    372\u001B[0m \u001B[38;5;66;03m# missing values.\u001B[39;00m\n\u001B[1;32m    373\u001B[0m estimator \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimator)(criterion\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcriterion)\n",
      "File \u001B[0;32m~/Desktop/Proiecte/venvs/llm_and_cognition_stats/lib/python3.12/site-packages/sklearn/utils/validation.py:2961\u001B[0m, in \u001B[0;36mvalidate_data\u001B[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001B[0m\n\u001B[1;32m   2959\u001B[0m         y \u001B[38;5;241m=\u001B[39m check_array(y, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_y_params)\n\u001B[1;32m   2960\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2961\u001B[0m         X, y \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_X_y\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcheck_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2962\u001B[0m     out \u001B[38;5;241m=\u001B[39m X, y\n\u001B[1;32m   2964\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m check_params\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mensure_2d\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m):\n",
      "File \u001B[0;32m~/Desktop/Proiecte/venvs/llm_and_cognition_stats/lib/python3.12/site-packages/sklearn/utils/validation.py:1387\u001B[0m, in \u001B[0;36mcheck_X_y\u001B[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001B[0m\n\u001B[1;32m   1368\u001B[0m ensure_all_finite \u001B[38;5;241m=\u001B[39m _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001B[1;32m   1370\u001B[0m X \u001B[38;5;241m=\u001B[39m check_array(\n\u001B[1;32m   1371\u001B[0m     X,\n\u001B[1;32m   1372\u001B[0m     accept_sparse\u001B[38;5;241m=\u001B[39maccept_sparse,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1384\u001B[0m     input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1385\u001B[0m )\n\u001B[0;32m-> 1387\u001B[0m y \u001B[38;5;241m=\u001B[39m \u001B[43m_check_y\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmulti_output\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmulti_output\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_numeric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my_numeric\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1389\u001B[0m check_consistent_length(X, y)\n\u001B[1;32m   1391\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m X, y\n",
      "File \u001B[0;32m~/Desktop/Proiecte/venvs/llm_and_cognition_stats/lib/python3.12/site-packages/sklearn/utils/validation.py:1397\u001B[0m, in \u001B[0;36m_check_y\u001B[0;34m(y, multi_output, y_numeric, estimator)\u001B[0m\n\u001B[1;32m   1395\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Isolated part of check_X_y dedicated to y validation\"\"\"\u001B[39;00m\n\u001B[1;32m   1396\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m multi_output:\n\u001B[0;32m-> 1397\u001B[0m     y \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_array\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1398\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1399\u001B[0m \u001B[43m        \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1400\u001B[0m \u001B[43m        \u001B[49m\u001B[43mensure_all_finite\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1401\u001B[0m \u001B[43m        \u001B[49m\u001B[43mensure_2d\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1402\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1403\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43my\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1404\u001B[0m \u001B[43m        \u001B[49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1405\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1406\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1407\u001B[0m     estimator_name \u001B[38;5;241m=\u001B[39m _check_estimator_name(estimator)\n",
      "File \u001B[0;32m~/Desktop/Proiecte/venvs/llm_and_cognition_stats/lib/python3.12/site-packages/sklearn/utils/validation.py:1107\u001B[0m, in \u001B[0;36mcheck_array\u001B[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[1;32m   1101\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1102\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound array with dim \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m expected <= 2.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1103\u001B[0m         \u001B[38;5;241m%\u001B[39m (array\u001B[38;5;241m.\u001B[39mndim, estimator_name)\n\u001B[1;32m   1104\u001B[0m     )\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ensure_all_finite:\n\u001B[0;32m-> 1107\u001B[0m     \u001B[43m_assert_all_finite\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1108\u001B[0m \u001B[43m        \u001B[49m\u001B[43marray\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1109\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1110\u001B[0m \u001B[43m        \u001B[49m\u001B[43mestimator_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[43m        \u001B[49m\u001B[43mallow_nan\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mensure_all_finite\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mallow-nan\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1112\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1114\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m copy:\n\u001B[1;32m   1115\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_numpy_namespace(xp):\n\u001B[1;32m   1116\u001B[0m         \u001B[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Proiecte/venvs/llm_and_cognition_stats/lib/python3.12/site-packages/sklearn/utils/validation.py:105\u001B[0m, in \u001B[0;36m_assert_all_finite\u001B[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001B[0m\n\u001B[1;32m    103\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_array_api \u001B[38;5;129;01mand\u001B[39;00m X\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m np\u001B[38;5;241m.\u001B[39mdtype(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobject\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m allow_nan:\n\u001B[1;32m    104\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _object_dtype_isnan(X)\u001B[38;5;241m.\u001B[39many():\n\u001B[0;32m--> 105\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput contains NaN\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    107\u001B[0m \u001B[38;5;66;03m# We need only consider float arrays, hence can early return for all else.\u001B[39;00m\n\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m xp\u001B[38;5;241m.\u001B[39misdtype(X\u001B[38;5;241m.\u001B[39mdtype, (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreal floating\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcomplex floating\u001B[39m\u001B[38;5;124m\"\u001B[39m)):\n",
      "\u001B[0;31mValueError\u001B[0m: Input contains NaN"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def get_features_2(train_text, validation_text, num_features=100, window=3, min_count=1, \n",
    "                   use_skipgram=True):\n",
    "\n",
    "    train_tokens = [text.split() for text in train_text]\n",
    "    validation_tokens = [text.split() for text in validation_text]\n",
    "\n",
    "    model = Word2Vec(sentences=train_tokens, \n",
    "                     vector_size=num_features, \n",
    "                     window=window, \n",
    "                     min_count=min_count,\n",
    "                     sg=1 if use_skipgram else 0)\n",
    "\n",
    "    def text_to_features(tokens):\n",
    "        features = []\n",
    "        for text in tokens:\n",
    "            text_vectors = [model.wv[word] for word in text if word in model.wv]\n",
    "\n",
    "            if not text_vectors:\n",
    "                features.append(np.zeros(num_features))\n",
    "            else:\n",
    "                features.append(np.mean(text_vectors, axis=0))\n",
    "        \n",
    "        return np.array(features)\n",
    "\n",
    "    train_features = text_to_features(train_tokens)\n",
    "    validation_features = text_to_features(validation_tokens)\n",
    "    \n",
    "    return train_features, validation_features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T08:00:57.548265885Z",
     "start_time": "2025-02-03T17:40:42.680696Z"
    }
   },
   "id": "f8d2886a2139a232"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Testing for num_features = 500, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.4432860717264387\n",
      "F1: 0.4294160581802838\n",
      "==================================================\n",
      "Testing for num_features = 500, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.439115929941618\n",
      "F1: 0.4261659088408784\n",
      "==================================================\n",
      "Testing for num_features = 500, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.44578815679733114\n",
      "F1: 0.4319493075392582\n",
      "==================================================\n",
      "Testing for num_features = 1000, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.43661384487072563\n",
      "F1: 0.42455545991975496\n",
      "==================================================\n",
      "Testing for num_features = 1000, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.44412010008340286\n",
      "F1: 0.4330192444337682\n",
      "==================================================\n",
      "Testing for num_features = 1000, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.44286905754795663\n",
      "F1: 0.42785978720642937\n",
      "==================================================\n",
      "Testing for num_features = 1500, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.4303586321934946\n",
      "F1: 0.4174864099036831\n",
      "==================================================\n",
      "Testing for num_features = 1500, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.4332777314428691\n",
      "F1: 0.42168533211699916\n",
      "==================================================\n",
      "Testing for num_features = 1500, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.43577981651376146\n",
      "F1: 0.42175699734111166\n",
      "==================================================\n",
      "Testing for num_features = 2000, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.43369474562135113\n",
      "F1: 0.4215857533751826\n",
      "==================================================\n",
      "Testing for num_features = 2000, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.43452877397831524\n",
      "F1: 0.42324516950324276\n",
      "==================================================\n",
      "Testing for num_features = 2000, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.427022518765638\n",
      "F1: 0.41411804868484176\n",
      "==================================================\n",
      "Testing for num_features = 2500, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.4274395329441201\n",
      "F1: 0.41539638247346194\n",
      "==================================================\n",
      "Testing for num_features = 2500, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.427022518765638\n",
      "F1: 0.4155331780654685\n",
      "==================================================\n",
      "Testing for num_features = 2500, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.4341117597998332\n",
      "F1: 0.42500823866578324\n"
     ]
    }
   ],
   "source": [
    "f = open(\"rf-titles-word2vec.txt\", \"w\")\n",
    "\n",
    "for num_features in [500, 1000, 1500, 2000, 2500]:\n",
    "    for num_estimators in [100, 150, 200]:\n",
    "        print(\"=\" * 50)\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "        print(f\"Testing for num_features = {num_features}, num_estimators = {num_estimators}\")\n",
    "        f.write(f\"Testing for num_features = {num_features}, num_estimators = {num_estimators}\\n\")\n",
    "\n",
    "        print(\"Creating text features...\")\n",
    "        f.write(\"Creating text features...\\n\")\n",
    "\n",
    "        train_titles_featurized, validation_titles_featurized = get_features_2(train_titles, validation_titles, num_features)\n",
    "\n",
    "        print(\"Training Random Forest...\")\n",
    "        f.write(\"Training Random Forest...\\n\")\n",
    "        random_forest = RandomForestClassifier(\n",
    "            n_estimators=num_estimators,\n",
    "            random_state=42)\n",
    "        random_forest.fit(train_titles_featurized, train_labels)\n",
    "\n",
    "        print(\"Getting predictions...\")\n",
    "        f.write(\"Getting predictions...\\n\")\n",
    "        predictions = random_forest.predict(validation_titles_featurized)\n",
    "        \n",
    "        print(\"Computing accuracy...\")\n",
    "        f.write(\"Computing accuracy...\\n\")\n",
    "        accuracy = accuracy_score(validation_labels, predictions)\n",
    "\n",
    "        print(\"Computing f1...\")\n",
    "        f.write(\"Computing f1...\\n\")\n",
    "        f1 = f1_score(validation_labels, predictions, average='weighted')\n",
    "\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        f.write(f\"Accuracy: {accuracy}\\n\")\n",
    "\n",
    "        print(f\"F1: {f1}\")\n",
    "        f.write(f\"F1: {f1}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T08:00:57.548873137Z",
     "start_time": "2025-02-03T17:40:42.684108Z"
    }
   },
   "id": "8f76e6bf35123dc6"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T08:00:57.556189326Z",
     "start_time": "2025-02-03T17:56:54.504992Z"
    }
   },
   "id": "7de02f00d54ed3d0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
