{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-03T17:37:46.312371Z",
     "start_time": "2025-02-03T17:37:46.300750Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# De aici: https://en.wiktionary.org/wiki/Category:Romanian_prefixes\n",
    "romanian_prefixes = [\n",
    "    # A\n",
    "    \"agro\", \"alt\", \"ante\", \"anti\", \"aorto\", \"arhi\", \"astro\",\n",
    "\n",
    "    # B\n",
    "    \"balano\",\n",
    "\n",
    "    # C\n",
    "    \"cardio\", \"carpo\", \"cosmo\",\n",
    "\n",
    "    # D\n",
    "    \"demono\", \"des\", \"dez\",\n",
    "\n",
    "    # F\n",
    "    \"franco\",\n",
    "\n",
    "    # G\n",
    "    \"gastro\", \"germano\", \"greco\",\n",
    "\n",
    "    # H\n",
    "    \"hecto\", \"hiper\",\n",
    "\n",
    "    # I\n",
    "    \"în\",\n",
    "\n",
    "    # K\n",
    "    \"kilo\",\n",
    "\n",
    "    # L\n",
    "    \"lexico\",\n",
    "\n",
    "    # M\n",
    "    \"mili\", \"muzico\",\n",
    "\n",
    "    # N\n",
    "    \"nano\", \"ne\",\n",
    "\n",
    "    # O\n",
    "    \"ori\", \"ornito\",\n",
    "\n",
    "    # P\n",
    "    \"pneumo\", \"pre\", \"prea\", \"proto\", \"pseudo\", \"psiho\",\n",
    "\n",
    "    # R\n",
    "    \"răs\", \"re\", \"rino\", \"ruso\",\n",
    "\n",
    "    # S\n",
    "    \"stră\", \"sub\",\n",
    "\n",
    "    # T\n",
    "    \"tehno\", \"teo\", \"termo\",\n",
    "\n",
    "    # V\n",
    "    \"vice\"\n",
    "]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T17:37:46.341472Z",
     "start_time": "2025-02-03T17:37:46.315438Z"
    }
   },
   "id": "48c4959a309f48ab"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cativa\n"
     ]
    }
   ],
   "source": [
    "def replace_i_prefix(word, prefixes):\n",
    "  for prefix in prefixes:\n",
    "    try:\n",
    "      if word.lower().startswith(prefix) and len(word) > len(prefix) and word[len(prefix):][0] in [\"î\", \"Î\"]:\n",
    "        first_letter = word[len(prefix):][0]\n",
    "        first_letter = \"i\" if first_letter == \"î\" else (\"I\" if first_letter == \"Î\" else first_letter)\n",
    "        word = prefix + first_letter + word[len(prefix) + 1:]\n",
    "\n",
    "    except:\n",
    "      print(word)\n",
    "\n",
    "  word = word.replace(\"î\", \"a\").replace(\"Î\", \"A\")\n",
    "\n",
    "  return word\n",
    "\n",
    "def no_diacritics(text, prefixes):\n",
    "\n",
    "  text = replace_i_prefix(text, prefixes)\n",
    "\n",
    "\n",
    "  text = text.replace(\"â\", \"i\")\n",
    "  text = text.replace(\"Â\", \"I\")\n",
    "  text = text.replace(\"ș\", \"s\")\n",
    "  text = text.replace(\"ş\", \"s\")\n",
    "  text = text.replace(\"Ș\", \"S\")\n",
    "  text = text.replace(\"Ş\", \"S\")\n",
    "  text = text.replace(\"ț\", \"t\")\n",
    "  text = text.replace(\"ţ\", \"t\")\n",
    "  text = text.replace(\"Ț\", \"T\")\n",
    "  text = text.replace(\"Ţ\", \"T\")\n",
    "\n",
    "  # If î is the first letter of the word, replace it with i\n",
    "  if text.startswith(\"î\"):\n",
    "    text = text.replace(\"î\", \"i\")\n",
    "  if text.startswith(\"Î\"):\n",
    "    text = text.replace(\"Î\", \"I\")\n",
    "  # If the last letter of the word is î, replace it with i\n",
    "  if text.endswith(\"î\"):\n",
    "    text = text.replace(\"î\", \"i\")\n",
    "  if text.endswith(\"Î\"):\n",
    "    text = text.replace(\"Î\", \"I\")\n",
    "  # Else replace î with a\n",
    "  if \"î\" in text:\n",
    "    text = text.replace(\"î\", \"a\")\n",
    "  # text = text.replace(\"î\", \"i\")\n",
    "  # text = text.replace(\"Î\", \"I\")\n",
    "  text = text.replace(\"ă\", \"a\")\n",
    "  text = text.replace(\"Ă\", \"A\")\n",
    "\n",
    "  return text\n",
    "\n",
    "\n",
    "# for key in moldavian_texts:\n",
    "#     for i in range(len(moldavian_texts[key])):\n",
    "#         moldavian_texts[key][i] = no_diacritics(moldavian_texts[key][i], romanian_prefixes)\n",
    "\n",
    "# for key in romanian_texts:\n",
    "#     for i in range(len(romanian_texts[key])):\n",
    "#         romanian_texts[key][i] = no_diacritics(romanian_texts[key][i], romanian_prefixes)\n",
    "\n",
    "# print(moldavian_texts[\"Sport\"][0])\n",
    "# print(romanian_texts['Stiri'][12])\n",
    "\n",
    "print(no_diacritics(\"cîțiva\", romanian_prefixes))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T17:37:46.342018Z",
     "start_time": "2025-02-03T17:37:46.317963Z"
    }
   },
   "id": "892a96c01a32732f"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "romanian=[\n",
    "    \"a\", \"abia\", \"acea\", \"aceasta\", \"această\", \"aceea\", \"aceeasi\", \"acei\",\n",
    "    \"aceia\", \"acel\", \"acela\", \"acelasi\", \"acele\", \"acelea\", \"acest\", \"acesta\",\n",
    "    \"aceste\", \"acestea\", \"acestei\", \"acestia\", \"acestui\", \"aceşti\", \"aceştia\",\n",
    "    \"acești\", \"aceștia\", \"acolo\", \"acord\", \"acum\", \"adica\", \"ai\", \"aia\",\n",
    "    \"aibă\", \"aici\", \"aiurea\", \"al\", \"ala\", \"alaturi\", \"ale\", \"alea\", \"alt\",\n",
    "    \"alta\", \"altceva\", \"altcineva\", \"alte\", \"altfel\", \"alti\", \"altii\", \"altul\",\n",
    "    \"alături\", \"am\", \"anume\", \"apoi\", \"ar\", \"are\", \"as\", \"asa\", \"asemenea\",\n",
    "    \"asta\", \"astazi\", \"astea\", \"astfel\", \"astăzi\", \"asupra\", \"atare\", \"atat\",\n",
    "    \"atata\", \"atatea\", \"atatia\", \"ati\", \"atit\", \"atita\", \"atitea\", \"atitia\",\n",
    "    \"atunci\", \"au\", \"avea\", \"avem\", \"aveţi\", \"aveți\", \"avut\", \"azi\", \"aş\",\n",
    "    \"aşadar\", \"aţi\", \"aș\", \"așadar\", \"ați\", \"b\", \"ba\", \"bine\", \"bucur\", \"bună\",\n",
    "    \"c\", \"ca\", \"cam\", \"cand\", \"capat\", \"care\", \"careia\", \"carora\", \"caruia\",\n",
    "    \"cat\", \"catre\", \"caut\", \"ce\", \"cea\", \"ceea\", \"cei\", \"ceilalti\", \"cel\",\n",
    "    \"cele\", \"celor\", \"ceva\", \"chiar\", \"ci\", \"cinci\", \"cind\", \"cine\", \"cineva\",\n",
    "    \"cit\", \"cita\", \"cite\", \"citeva\", \"citi\", \"câțiva\", \"conform\", \"contra\",\n",
    "    \"cu\", \"cui\", \"cum\", \"cumva\", \"curând\", \"curînd\", \"când\", \"cât\", \"câte\",\n",
    "    \"câtva\", \"câţi\", \"câți\", \"cînd\", \"cît\", \"cîte\", \"cîtva\", \"cîţi\", \"cîți\",\n",
    "    \"că\", \"căci\", \"cărei\", \"căror\", \"cărui\", \"către\", \"d\", \"da\", \"daca\",\n",
    "    \"dacă\", \"dar\", \"dat\", \"datorită\", \"dată\", \"dau\", \"de\", \"deasupra\", \"deci\",\n",
    "    \"decit\", \"degraba\", \"deja\", \"deoarece\", \"departe\", \"desi\", \"despre\",\n",
    "    \"deşi\", \"deși\", \"din\", \"dinaintea\", \"dintr\", \"dintr-\", \"dintre\", \"doar\",\n",
    "    \"doi\", \"doilea\", \"două\", \"drept\", \"dupa\", \"după\", \"dă\", \"e\", \"ea\", \"ei\",\n",
    "    \"el\", \"ele\", \"era\", \"eram\", \"este\", \"eu\", \"exact\", \"eşti\", \"ești\", \"f\",\n",
    "    \"face\", \"fara\", \"fata\", \"fel\", \"fi\", \"fie\", \"fiecare\", \"fii\", \"fim\", \"fiu\",\n",
    "    \"fiţi\", \"fiți\", \"foarte\", \"fost\", \"frumos\", \"fără\", \"g\", \"geaba\", \"graţie\",\n",
    "    \"grație\", \"h\", \"halbă\", \"i\", \"ia\", \"iar\", \"ieri\", \"ii\", \"il\", \"imi\", \"in\",\n",
    "    \"inainte\", \"inapoi\", \"inca\", \"incit\", \"insa\", \"intr\", \"intre\", \"isi\",\n",
    "    \"iti\", \"j\", \"k\", \"l\", \"la\", \"le\", \"li\", \"lor\", \"lui\", \"lângă\", \"lîngă\",\n",
    "    \"m\", \"ma\", \"mai\", \"mare\", \"mea\", \"mei\", \"mele\", \"mereu\", \"meu\", \"mi\",\n",
    "    \"mie\", \"mine\", \"mod\", \"mult\", \"multa\", \"multe\", \"multi\", \"multă\", \"mulţi\",\n",
    "    \"mulţumesc\", \"mulți\", \"mulțumesc\", \"mâine\", \"mîine\", \"mă\", \"n\", \"ne\",\n",
    "    \"nevoie\", \"ni\", \"nici\", \"niciodata\", \"nicăieri\", \"nimeni\", \"nimeri\",\n",
    "    \"nimic\", \"niste\", \"nişte\", \"niște\", \"noastre\", \"noastră\", \"noi\", \"noroc\",\n",
    "    \"nostri\", \"nostru\", \"nou\", \"noua\", \"nouă\", \"noştri\", \"noștri\", \"nu\",\n",
    "    \"numai\", \"o\", \"opt\", \"or\", \"ori\", \"oricare\", \"orice\", \"oricine\", \"oricum\",\n",
    "    \"oricând\", \"oricât\", \"oricînd\", \"oricît\", \"oriunde\", \"p\", \"pai\", \"parca\",\n",
    "    \"patra\", \"patru\", \"patrulea\", \"pe\", \"pentru\", \"peste\", \"pic\", \"pina\",\n",
    "    \"plus\", \"poate\", \"pot\", \"prea\", \"prima\", \"primul\", \"prin\", \"printr-\",\n",
    "    \"putini\", \"puţin\", \"puţina\", \"puţină\", \"puțin\", \"puțina\", \"puțină\", \"până\",\n",
    "    \"pînă\", \"r\", \"rog\", \"s\", \"sa\", \"sa-mi\", \"sa-ti\", \"sai\", \"sale\", \"sau\",\n",
    "    \"se\", \"si\", \"sint\", \"sintem\", \"spate\", \"spre\", \"sub\", \"sunt\", \"suntem\",\n",
    "    \"sunteţi\", \"sunteți\", \"sus\", \"sută\", \"sînt\", \"sîntem\", \"sînteţi\",\n",
    "    \"sînteți\", \"să\", \"săi\", \"său\", \"t\", \"ta\", \"tale\", \"te\", \"ti\", \"timp\",\n",
    "    \"tine\", \"toata\", \"toate\", \"toată\", \"tocmai\", \"tot\", \"toti\", \"totul\",\n",
    "    \"totusi\", \"totuşi\", \"totuși\", \"toţi\", \"toți\", \"trei\", \"treia\", \"treilea\",\n",
    "    \"tu\", \"tuturor\", \"tăi\", \"tău\", \"u\", \"ul\", \"ului\", \"un\", \"una\", \"unde\",\n",
    "    \"undeva\", \"unei\", \"uneia\", \"unele\", \"uneori\", \"unii\", \"unor\", \"unora\",\n",
    "    \"unu\", \"unui\", \"unuia\", \"unul\", \"v\", \"va\", \"vi\", \"voastre\", \"voastră\",\n",
    "    \"voi\", \"vom\", \"vor\", \"vostru\", \"vouă\", \"voştri\", \"voștri\", \"vreme\", \"vreo\",\n",
    "    \"vreun\", \"vă\", \"x\", \"z\", \"zece\", \"zero\", \"zi\", \"zice\", \"îi\", \"îl\", \"îmi\",\n",
    "    \"împotriva\", \"în\", \"înainte\", \"înaintea\", \"încotro\", \"încât\", \"încît\",\n",
    "    \"între\", \"întrucât\", \"întrucît\", \"îţi\", \"îți\", \"ăla\", \"ălea\", \"ăsta\",\n",
    "    \"ăstea\", \"ăştia\", \"ăștia\", \"şapte\", \"şase\", \"şi\", \"ştiu\", \"ţi\", \"ţie\",\n",
    "    \"șapte\", \"șase\", \"și\", \"știu\", \"ți\", \"ție\"\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T17:37:46.342589Z",
     "start_time": "2025-02-03T17:37:46.330807Z"
    }
   },
   "id": "6f424384dbf01e6b"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# # Get all the words from the stop words list and apply the same transformation\n",
    "stop_words = romanian\n",
    "for i in range(len(stop_words)):\n",
    "    stop_words[i] = no_diacritics(stop_words[i], romanian_prefixes)\n",
    "\n",
    "stop_words = list(set(stop_words))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T17:37:46.360044Z",
     "start_time": "2025-02-03T17:37:46.346187Z"
    }
   },
   "id": "a152024aa25a71cb"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Distribution:\n",
      "Banat: 1000\n",
      "Ardeal: 1000\n",
      "Muntenia: 1000\n",
      "Serbia: 1000\n",
      "Moldova: 1000\n",
      "Oltenia: 1000\n",
      "Ucraina: 1000\n",
      "Dobrogea: 965\n",
      "Spania: 723\n",
      "Maramures: 656\n",
      "Canada_EN: 641\n",
      "Crisana: 579\n",
      "Germania: 500\n",
      "UK: 499\n",
      "Bucovina: 428\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_label_distribution(dataset_folder):\n",
    "    json_files = [f for f in os.listdir(dataset_folder) if f.endswith('.json')]\n",
    "    \n",
    "    dataframes = []\n",
    "    \n",
    "    for filename in json_files:\n",
    "        filepath = os.path.join(dataset_folder, filename)\n",
    "        \n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if 200 <= len(data) <= 1000:\n",
    "            df = pd.DataFrame(data)\n",
    "            dataframes.append(df)\n",
    "        elif len(data) > 1000:\n",
    "            df = pd.DataFrame(data).sample(n=1000, random_state=42)\n",
    "            dataframes.append(df)\n",
    "    \n",
    "    # Combine dataframes\n",
    "    if dataframes:\n",
    "        final_dataframe = pd.concat(dataframes, ignore_index=True)\n",
    "        \n",
    "        # Count label distribution\n",
    "        label_counts = final_dataframe['label'].value_counts()\n",
    "        \n",
    "        print(\"Label Distribution:\")\n",
    "        for label, count in label_counts.items():\n",
    "            print(f\"{label}: {count}\")\n",
    "        \n",
    "        return final_dataframe\n",
    "    else:\n",
    "        print(\"No files met the criteria.\")\n",
    "        return None\n",
    "\n",
    "# Run the analysis\n",
    "dataset_folder = 'Dataset'\n",
    "data = analyze_label_distribution(dataset_folder)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T17:37:46.706394Z",
     "start_time": "2025-02-03T17:37:46.363244Z"
    }
   },
   "id": "4f053a001f19b23e"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def get_features_1(train_text, validation_text, num_features):\n",
    "    tfidf = TfidfVectorizer(max_features=num_features)\n",
    "\n",
    "    train_text_tfidf = tfidf.fit_transform(train_text)\n",
    "\n",
    "    validation_text_tfidf = tfidf.transform(validation_text)\n",
    "\n",
    "    return train_text_tfidf.toarray(), validation_text_tfidf.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T17:37:46.706842Z",
     "start_time": "2025-02-03T17:37:46.704204Z"
    }
   },
   "id": "5f9f92824568c261"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ro_core_news_sm\")\n",
    "\n",
    "def preprocess(text):\n",
    "    text = no_diacritics(text, romanian_prefixes)\n",
    "    text = ' '.join(word for word in text.split() if word.lower() not in stop_words)\n",
    "    doc = nlp(text)\n",
    "    text = ' '.join(token.lemma_ for token in doc if not token.is_stop)\n",
    "    \n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T17:37:46.907750Z",
     "start_time": "2025-02-03T17:37:46.707659Z"
    }
   },
   "id": "ee90ddb829549a79"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "396     primar comună Birna , implica antr un accident...\n2955    tactică folosi hotii romină Germania raspunde ...\n3647    Gheorghe Flutur anunta construire ret distribu...\n357     promisiune electoral vs . realitate . strada C...\n1323    partid popular sustine campanie antirominească...\n                              ...                        \n6648    Petre Pirvu , primar oras Zimnicea , sustine d...\n9065          adolescent Margineni disparut , pleca acasă\n6772      asteaptă program bogat participanti • libertate\n7969    23 artisti Moldova , Olanda , Serbia , Polonia...\n6101    constructor drum Vulcanii Noroiosi putea afla ...\nName: title, Length: 2398, dtype: object"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[data['title'].str.strip() != ''].dropna(subset=['title'])\n",
    "\n",
    "titles = data['title']\n",
    "labels = data['label']\n",
    "\n",
    "train_titles, validation_titles, train_labels, validation_labels = train_test_split(titles, \n",
    "                                                                                  labels, test_size=0.2, random_state=42)\n",
    "\n",
    "train_titles.apply(preprocess)\n",
    "validation_titles.apply(preprocess)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T17:38:27.628707Z",
     "start_time": "2025-02-03T17:37:46.911557Z"
    }
   },
   "id": "7abfb665b4c69d9b"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Testing for num_features = 500, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.6676396997497915\n",
      "F1: 0.669130441524788\n",
      "==================================================\n",
      "Testing for num_features = 500, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.6663886572143453\n",
      "F1: 0.6679513609992888\n",
      "==================================================\n",
      "Testing for num_features = 500, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.6659716430358632\n",
      "F1: 0.6674923912070897\n",
      "==================================================\n",
      "Testing for num_features = 1000, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.6959966638865721\n",
      "F1: 0.697879147301196\n",
      "==================================================\n",
      "Testing for num_features = 1000, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.6976647206005004\n",
      "F1: 0.6993794344645159\n",
      "==================================================\n",
      "Testing for num_features = 1000, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.6959966638865721\n",
      "F1: 0.6974290278462504\n",
      "==================================================\n",
      "Testing for num_features = 1500, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.7085070892410342\n",
      "F1: 0.710720762033631\n",
      "==================================================\n",
      "Testing for num_features = 1500, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.7101751459549625\n",
      "F1: 0.7121151894225811\n",
      "==================================================\n",
      "Testing for num_features = 1500, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.7101751459549625\n",
      "F1: 0.7124416113449398\n",
      "==================================================\n",
      "Testing for num_features = 2000, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.7110091743119266\n",
      "F1: 0.7133449837758296\n",
      "==================================================\n",
      "Testing for num_features = 2000, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.7155963302752294\n",
      "F1: 0.7175309415686627\n",
      "==================================================\n",
      "Testing for num_features = 2000, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.7185154295246038\n",
      "F1: 0.7206124490552144\n",
      "==================================================\n",
      "Testing for num_features = 2500, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.7201834862385321\n",
      "F1: 0.72211739408933\n",
      "==================================================\n",
      "Testing for num_features = 2500, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.7206005004170142\n",
      "F1: 0.7234259305795875\n",
      "==================================================\n",
      "Testing for num_features = 2500, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.7214345287739783\n",
      "F1: 0.7241042877737481\n"
     ]
    }
   ],
   "source": [
    "f = open(\"knn-titles-tfidf.txt\", \"w\")\n",
    "\n",
    "for num_features in [500, 1000, 1500, 2000, 2500]:\n",
    "    for num_estimators in [100, 150, 200]:\n",
    "        print(\"=\" * 50)\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "        print(f\"Testing for num_features = {num_features}, num_estimators = {num_estimators}\")\n",
    "        f.write(f\"Testing for num_features = {num_features}, num_estimators = {num_estimators}\\n\")\n",
    "\n",
    "        print(\"Creating text features...\")\n",
    "        f.write(\"Creating text features...\\n\")\n",
    "\n",
    "        train_titles_featurized, validation_titles_featurized = get_features_1(train_titles, validation_titles, num_features)\n",
    "\n",
    "        print(\"Training Random Forest...\")\n",
    "        f.write(\"Training Random Forest...\\n\")\n",
    "        random_forest = RandomForestClassifier(\n",
    "            n_estimators=num_estimators,\n",
    "            random_state=42)\n",
    "        random_forest.fit(train_titles_featurized, train_labels)\n",
    "\n",
    "        print(\"Getting predictions...\")\n",
    "        f.write(\"Getting predictions...\\n\")\n",
    "        predictions = random_forest.predict(validation_titles_featurized)\n",
    "        \n",
    "        print(\"Computing accuracy...\")\n",
    "        f.write(\"Computing accuracy...\")\n",
    "        accuracy = accuracy_score(validation_labels, predictions)\n",
    "\n",
    "        print(\"Computing f1...\")\n",
    "        f.write(\"Computing f1...\\n\")\n",
    "        f1 = f1_score(validation_labels, predictions, average='weighted')\n",
    "\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        f.write(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "        print(f\"F1: {f1}\")\n",
    "        f.write(f\"F1: {f1}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T17:40:42.677942Z",
     "start_time": "2025-02-03T17:38:27.630882Z"
    }
   },
   "id": "3a7cc62f2cd35f2c"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def get_features_2(train_text, validation_text, num_features=100, window=3, min_count=1, \n",
    "                   use_skipgram=True):\n",
    "\n",
    "    train_tokens = [text.split() for text in train_text]\n",
    "    validation_tokens = [text.split() for text in validation_text]\n",
    "    \n",
    "    # Train Word2Vec model\n",
    "    model = Word2Vec(sentences=train_tokens, \n",
    "                     vector_size=num_features, \n",
    "                     window=window, \n",
    "                     min_count=min_count,\n",
    "                     sg=1 if use_skipgram else 0)  # 1 for skip-gram, 0 for CBOW\n",
    "    \n",
    "    # Function to convert text to Word2Vec features\n",
    "    def text_to_features(tokens):\n",
    "        features = []\n",
    "        for text in tokens:\n",
    "            # Calculate mean word vector for the text\n",
    "            text_vectors = [model.wv[word] for word in text if word in model.wv]\n",
    "            \n",
    "            # If no words found, use zero vector\n",
    "            if not text_vectors:\n",
    "                features.append(np.zeros(num_features))\n",
    "            else:\n",
    "                features.append(np.mean(text_vectors, axis=0))\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    # Convert train and validation texts to features\n",
    "    train_features = text_to_features(train_tokens)\n",
    "    validation_features = text_to_features(validation_tokens)\n",
    "    \n",
    "    return train_features, validation_features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T17:40:42.683218Z",
     "start_time": "2025-02-03T17:40:42.680696Z"
    }
   },
   "id": "f8d2886a2139a232"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Testing for num_features = 500, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.4432860717264387\n",
      "F1: 0.4294160581802838\n",
      "==================================================\n",
      "Testing for num_features = 500, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.439115929941618\n",
      "F1: 0.4261659088408784\n",
      "==================================================\n",
      "Testing for num_features = 500, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.44578815679733114\n",
      "F1: 0.4319493075392582\n",
      "==================================================\n",
      "Testing for num_features = 1000, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.43661384487072563\n",
      "F1: 0.42455545991975496\n",
      "==================================================\n",
      "Testing for num_features = 1000, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.44412010008340286\n",
      "F1: 0.4330192444337682\n",
      "==================================================\n",
      "Testing for num_features = 1000, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.44286905754795663\n",
      "F1: 0.42785978720642937\n",
      "==================================================\n",
      "Testing for num_features = 1500, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.4303586321934946\n",
      "F1: 0.4174864099036831\n",
      "==================================================\n",
      "Testing for num_features = 1500, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.4332777314428691\n",
      "F1: 0.42168533211699916\n",
      "==================================================\n",
      "Testing for num_features = 1500, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.43577981651376146\n",
      "F1: 0.42175699734111166\n",
      "==================================================\n",
      "Testing for num_features = 2000, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.43369474562135113\n",
      "F1: 0.4215857533751826\n",
      "==================================================\n",
      "Testing for num_features = 2000, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.43452877397831524\n",
      "F1: 0.42324516950324276\n",
      "==================================================\n",
      "Testing for num_features = 2000, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.427022518765638\n",
      "F1: 0.41411804868484176\n",
      "==================================================\n",
      "Testing for num_features = 2500, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.4274395329441201\n",
      "F1: 0.41539638247346194\n",
      "==================================================\n",
      "Testing for num_features = 2500, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.427022518765638\n",
      "F1: 0.4155331780654685\n",
      "==================================================\n",
      "Testing for num_features = 2500, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.4341117597998332\n",
      "F1: 0.42500823866578324\n"
     ]
    }
   ],
   "source": [
    "f = open(\"knn-titles-word2vec.txt\", \"w\")\n",
    "\n",
    "for num_features in [500, 1000, 1500, 2000, 2500]:\n",
    "    for num_estimators in [100, 150, 200]:\n",
    "        print(\"=\" * 50)\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "        print(f\"Testing for num_features = {num_features}, num_estimators = {num_estimators}\")\n",
    "        f.write(f\"Testing for num_features = {num_features}, num_estimators = {num_estimators}\\n\")\n",
    "\n",
    "        print(\"Creating text features...\")\n",
    "        f.write(\"Creating text features...\\n\")\n",
    "\n",
    "        train_titles_featurized, validation_titles_featurized = get_features_2(train_titles, validation_titles, num_features)\n",
    "\n",
    "        print(\"Training Random Forest...\")\n",
    "        f.write(\"Training Random Forest...\\n\")\n",
    "        random_forest = RandomForestClassifier(\n",
    "            n_estimators=num_estimators,\n",
    "            random_state=42)\n",
    "        random_forest.fit(train_titles_featurized, train_labels)\n",
    "\n",
    "        print(\"Getting predictions...\")\n",
    "        f.write(\"Getting predictions...\\n\")\n",
    "        predictions = random_forest.predict(validation_titles_featurized)\n",
    "        \n",
    "        print(\"Computing accuracy...\")\n",
    "        f.write(\"Computing accuracy...\\n\")\n",
    "        accuracy = accuracy_score(validation_labels, predictions)\n",
    "\n",
    "        print(\"Computing f1...\")\n",
    "        f.write(\"Computing f1...\\n\")\n",
    "        f1 = f1_score(validation_labels, predictions, average='weighted')\n",
    "\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        f.write(f\"Accuracy: {accuracy}\\n\")\n",
    "\n",
    "        print(f\"F1: {f1}\")\n",
    "        f.write(f\"F1: {f1}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T17:56:54.506018Z",
     "start_time": "2025-02-03T17:40:42.684108Z"
    }
   },
   "id": "8f76e6bf35123dc6"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T17:56:54.506565Z",
     "start_time": "2025-02-03T17:56:54.504992Z"
    }
   },
   "id": "7de02f00d54ed3d0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
