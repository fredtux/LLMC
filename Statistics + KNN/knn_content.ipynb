{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-03T17:58:37.266840Z",
     "start_time": "2025-02-03T17:58:35.959122Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# De aici: https://en.wiktionary.org/wiki/Category:Romanian_prefixes\n",
    "romanian_prefixes = [\n",
    "    # A\n",
    "    \"agro\", \"alt\", \"ante\", \"anti\", \"aorto\", \"arhi\", \"astro\",\n",
    "\n",
    "    # B\n",
    "    \"balano\",\n",
    "\n",
    "    # C\n",
    "    \"cardio\", \"carpo\", \"cosmo\",\n",
    "\n",
    "    # D\n",
    "    \"demono\", \"des\", \"dez\",\n",
    "\n",
    "    # F\n",
    "    \"franco\",\n",
    "\n",
    "    # G\n",
    "    \"gastro\", \"germano\", \"greco\",\n",
    "\n",
    "    # H\n",
    "    \"hecto\", \"hiper\",\n",
    "\n",
    "    # I\n",
    "    \"în\",\n",
    "\n",
    "    # K\n",
    "    \"kilo\",\n",
    "\n",
    "    # L\n",
    "    \"lexico\",\n",
    "\n",
    "    # M\n",
    "    \"mili\", \"muzico\",\n",
    "\n",
    "    # N\n",
    "    \"nano\", \"ne\",\n",
    "\n",
    "    # O\n",
    "    \"ori\", \"ornito\",\n",
    "\n",
    "    # P\n",
    "    \"pneumo\", \"pre\", \"prea\", \"proto\", \"pseudo\", \"psiho\",\n",
    "\n",
    "    # R\n",
    "    \"răs\", \"re\", \"rino\", \"ruso\",\n",
    "\n",
    "    # S\n",
    "    \"stră\", \"sub\",\n",
    "\n",
    "    # T\n",
    "    \"tehno\", \"teo\", \"termo\",\n",
    "\n",
    "    # V\n",
    "    \"vice\"\n",
    "]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T17:58:37.271665Z",
     "start_time": "2025-02-03T17:58:37.268447Z"
    }
   },
   "id": "48c4959a309f48ab"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cativa\n"
     ]
    }
   ],
   "source": [
    "def replace_i_prefix(word, prefixes):\n",
    "  for prefix in prefixes:\n",
    "    try:\n",
    "      if word.lower().startswith(prefix) and len(word) > len(prefix) and word[len(prefix):][0] in [\"î\", \"Î\"]:\n",
    "        first_letter = word[len(prefix):][0]\n",
    "        first_letter = \"i\" if first_letter == \"î\" else (\"I\" if first_letter == \"Î\" else first_letter)\n",
    "        word = prefix + first_letter + word[len(prefix) + 1:]\n",
    "\n",
    "    except:\n",
    "      print(word)\n",
    "\n",
    "  word = word.replace(\"î\", \"a\").replace(\"Î\", \"A\")\n",
    "\n",
    "  return word\n",
    "\n",
    "def no_diacritics(text, prefixes):\n",
    "\n",
    "  text = replace_i_prefix(text, prefixes)\n",
    "\n",
    "\n",
    "  text = text.replace(\"â\", \"i\")\n",
    "  text = text.replace(\"Â\", \"I\")\n",
    "  text = text.replace(\"ș\", \"s\")\n",
    "  text = text.replace(\"ş\", \"s\")\n",
    "  text = text.replace(\"Ș\", \"S\")\n",
    "  text = text.replace(\"Ş\", \"S\")\n",
    "  text = text.replace(\"ț\", \"t\")\n",
    "  text = text.replace(\"ţ\", \"t\")\n",
    "  text = text.replace(\"Ț\", \"T\")\n",
    "  text = text.replace(\"Ţ\", \"T\")\n",
    "\n",
    "  # If î is the first letter of the word, replace it with i\n",
    "  if text.startswith(\"î\"):\n",
    "    text = text.replace(\"î\", \"i\")\n",
    "  if text.startswith(\"Î\"):\n",
    "    text = text.replace(\"Î\", \"I\")\n",
    "  # If the last letter of the word is î, replace it with i\n",
    "  if text.endswith(\"î\"):\n",
    "    text = text.replace(\"î\", \"i\")\n",
    "  if text.endswith(\"Î\"):\n",
    "    text = text.replace(\"Î\", \"I\")\n",
    "  # Else replace î with a\n",
    "  if \"î\" in text:\n",
    "    text = text.replace(\"î\", \"a\")\n",
    "  # text = text.replace(\"î\", \"i\")\n",
    "  # text = text.replace(\"Î\", \"I\")\n",
    "  text = text.replace(\"ă\", \"a\")\n",
    "  text = text.replace(\"Ă\", \"A\")\n",
    "\n",
    "  return text\n",
    "\n",
    "\n",
    "# for key in moldavian_texts:\n",
    "#     for i in range(len(moldavian_texts[key])):\n",
    "#         moldavian_texts[key][i] = no_diacritics(moldavian_texts[key][i], romanian_prefixes)\n",
    "\n",
    "# for key in romanian_texts:\n",
    "#     for i in range(len(romanian_texts[key])):\n",
    "#         romanian_texts[key][i] = no_diacritics(romanian_texts[key][i], romanian_prefixes)\n",
    "\n",
    "# print(moldavian_texts[\"Sport\"][0])\n",
    "# print(romanian_texts['Stiri'][12])\n",
    "\n",
    "print(no_diacritics(\"cîțiva\", romanian_prefixes))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T17:58:37.276269Z",
     "start_time": "2025-02-03T17:58:37.272969Z"
    }
   },
   "id": "892a96c01a32732f"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "romanian=[\n",
    "    \"a\", \"abia\", \"acea\", \"aceasta\", \"această\", \"aceea\", \"aceeasi\", \"acei\",\n",
    "    \"aceia\", \"acel\", \"acela\", \"acelasi\", \"acele\", \"acelea\", \"acest\", \"acesta\",\n",
    "    \"aceste\", \"acestea\", \"acestei\", \"acestia\", \"acestui\", \"aceşti\", \"aceştia\",\n",
    "    \"acești\", \"aceștia\", \"acolo\", \"acord\", \"acum\", \"adica\", \"ai\", \"aia\",\n",
    "    \"aibă\", \"aici\", \"aiurea\", \"al\", \"ala\", \"alaturi\", \"ale\", \"alea\", \"alt\",\n",
    "    \"alta\", \"altceva\", \"altcineva\", \"alte\", \"altfel\", \"alti\", \"altii\", \"altul\",\n",
    "    \"alături\", \"am\", \"anume\", \"apoi\", \"ar\", \"are\", \"as\", \"asa\", \"asemenea\",\n",
    "    \"asta\", \"astazi\", \"astea\", \"astfel\", \"astăzi\", \"asupra\", \"atare\", \"atat\",\n",
    "    \"atata\", \"atatea\", \"atatia\", \"ati\", \"atit\", \"atita\", \"atitea\", \"atitia\",\n",
    "    \"atunci\", \"au\", \"avea\", \"avem\", \"aveţi\", \"aveți\", \"avut\", \"azi\", \"aş\",\n",
    "    \"aşadar\", \"aţi\", \"aș\", \"așadar\", \"ați\", \"b\", \"ba\", \"bine\", \"bucur\", \"bună\",\n",
    "    \"c\", \"ca\", \"cam\", \"cand\", \"capat\", \"care\", \"careia\", \"carora\", \"caruia\",\n",
    "    \"cat\", \"catre\", \"caut\", \"ce\", \"cea\", \"ceea\", \"cei\", \"ceilalti\", \"cel\",\n",
    "    \"cele\", \"celor\", \"ceva\", \"chiar\", \"ci\", \"cinci\", \"cind\", \"cine\", \"cineva\",\n",
    "    \"cit\", \"cita\", \"cite\", \"citeva\", \"citi\", \"câțiva\", \"conform\", \"contra\",\n",
    "    \"cu\", \"cui\", \"cum\", \"cumva\", \"curând\", \"curînd\", \"când\", \"cât\", \"câte\",\n",
    "    \"câtva\", \"câţi\", \"câți\", \"cînd\", \"cît\", \"cîte\", \"cîtva\", \"cîţi\", \"cîți\",\n",
    "    \"că\", \"căci\", \"cărei\", \"căror\", \"cărui\", \"către\", \"d\", \"da\", \"daca\",\n",
    "    \"dacă\", \"dar\", \"dat\", \"datorită\", \"dată\", \"dau\", \"de\", \"deasupra\", \"deci\",\n",
    "    \"decit\", \"degraba\", \"deja\", \"deoarece\", \"departe\", \"desi\", \"despre\",\n",
    "    \"deşi\", \"deși\", \"din\", \"dinaintea\", \"dintr\", \"dintr-\", \"dintre\", \"doar\",\n",
    "    \"doi\", \"doilea\", \"două\", \"drept\", \"dupa\", \"după\", \"dă\", \"e\", \"ea\", \"ei\",\n",
    "    \"el\", \"ele\", \"era\", \"eram\", \"este\", \"eu\", \"exact\", \"eşti\", \"ești\", \"f\",\n",
    "    \"face\", \"fara\", \"fata\", \"fel\", \"fi\", \"fie\", \"fiecare\", \"fii\", \"fim\", \"fiu\",\n",
    "    \"fiţi\", \"fiți\", \"foarte\", \"fost\", \"frumos\", \"fără\", \"g\", \"geaba\", \"graţie\",\n",
    "    \"grație\", \"h\", \"halbă\", \"i\", \"ia\", \"iar\", \"ieri\", \"ii\", \"il\", \"imi\", \"in\",\n",
    "    \"inainte\", \"inapoi\", \"inca\", \"incit\", \"insa\", \"intr\", \"intre\", \"isi\",\n",
    "    \"iti\", \"j\", \"k\", \"l\", \"la\", \"le\", \"li\", \"lor\", \"lui\", \"lângă\", \"lîngă\",\n",
    "    \"m\", \"ma\", \"mai\", \"mare\", \"mea\", \"mei\", \"mele\", \"mereu\", \"meu\", \"mi\",\n",
    "    \"mie\", \"mine\", \"mod\", \"mult\", \"multa\", \"multe\", \"multi\", \"multă\", \"mulţi\",\n",
    "    \"mulţumesc\", \"mulți\", \"mulțumesc\", \"mâine\", \"mîine\", \"mă\", \"n\", \"ne\",\n",
    "    \"nevoie\", \"ni\", \"nici\", \"niciodata\", \"nicăieri\", \"nimeni\", \"nimeri\",\n",
    "    \"nimic\", \"niste\", \"nişte\", \"niște\", \"noastre\", \"noastră\", \"noi\", \"noroc\",\n",
    "    \"nostri\", \"nostru\", \"nou\", \"noua\", \"nouă\", \"noştri\", \"noștri\", \"nu\",\n",
    "    \"numai\", \"o\", \"opt\", \"or\", \"ori\", \"oricare\", \"orice\", \"oricine\", \"oricum\",\n",
    "    \"oricând\", \"oricât\", \"oricînd\", \"oricît\", \"oriunde\", \"p\", \"pai\", \"parca\",\n",
    "    \"patra\", \"patru\", \"patrulea\", \"pe\", \"pentru\", \"peste\", \"pic\", \"pina\",\n",
    "    \"plus\", \"poate\", \"pot\", \"prea\", \"prima\", \"primul\", \"prin\", \"printr-\",\n",
    "    \"putini\", \"puţin\", \"puţina\", \"puţină\", \"puțin\", \"puțina\", \"puțină\", \"până\",\n",
    "    \"pînă\", \"r\", \"rog\", \"s\", \"sa\", \"sa-mi\", \"sa-ti\", \"sai\", \"sale\", \"sau\",\n",
    "    \"se\", \"si\", \"sint\", \"sintem\", \"spate\", \"spre\", \"sub\", \"sunt\", \"suntem\",\n",
    "    \"sunteţi\", \"sunteți\", \"sus\", \"sută\", \"sînt\", \"sîntem\", \"sînteţi\",\n",
    "    \"sînteți\", \"să\", \"săi\", \"său\", \"t\", \"ta\", \"tale\", \"te\", \"ti\", \"timp\",\n",
    "    \"tine\", \"toata\", \"toate\", \"toată\", \"tocmai\", \"tot\", \"toti\", \"totul\",\n",
    "    \"totusi\", \"totuşi\", \"totuși\", \"toţi\", \"toți\", \"trei\", \"treia\", \"treilea\",\n",
    "    \"tu\", \"tuturor\", \"tăi\", \"tău\", \"u\", \"ul\", \"ului\", \"un\", \"una\", \"unde\",\n",
    "    \"undeva\", \"unei\", \"uneia\", \"unele\", \"uneori\", \"unii\", \"unor\", \"unora\",\n",
    "    \"unu\", \"unui\", \"unuia\", \"unul\", \"v\", \"va\", \"vi\", \"voastre\", \"voastră\",\n",
    "    \"voi\", \"vom\", \"vor\", \"vostru\", \"vouă\", \"voştri\", \"voștri\", \"vreme\", \"vreo\",\n",
    "    \"vreun\", \"vă\", \"x\", \"z\", \"zece\", \"zero\", \"zi\", \"zice\", \"îi\", \"îl\", \"îmi\",\n",
    "    \"împotriva\", \"în\", \"înainte\", \"înaintea\", \"încotro\", \"încât\", \"încît\",\n",
    "    \"între\", \"întrucât\", \"întrucît\", \"îţi\", \"îți\", \"ăla\", \"ălea\", \"ăsta\",\n",
    "    \"ăstea\", \"ăştia\", \"ăștia\", \"şapte\", \"şase\", \"şi\", \"ştiu\", \"ţi\", \"ţie\",\n",
    "    \"șapte\", \"șase\", \"și\", \"știu\", \"ți\", \"ție\"\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T17:58:37.285971Z",
     "start_time": "2025-02-03T17:58:37.281241Z"
    }
   },
   "id": "6f424384dbf01e6b"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# # Get all the words from the stop words list and apply the same transformation\n",
    "stop_words = romanian\n",
    "for i in range(len(stop_words)):\n",
    "    stop_words[i] = no_diacritics(stop_words[i], romanian_prefixes)\n",
    "\n",
    "stop_words = list(set(stop_words))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T17:58:37.300116Z",
     "start_time": "2025-02-03T17:58:37.286693Z"
    }
   },
   "id": "a152024aa25a71cb"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Distribution:\n",
      "Banat: 1000\n",
      "Ardeal: 1000\n",
      "Muntenia: 1000\n",
      "Serbia: 1000\n",
      "Moldova: 1000\n",
      "Oltenia: 1000\n",
      "Ucraina: 1000\n",
      "Dobrogea: 965\n",
      "Spania: 723\n",
      "Maramures: 656\n",
      "Canada_EN: 641\n",
      "Crisana: 579\n",
      "Germania: 500\n",
      "UK: 499\n",
      "Bucovina: 428\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_label_distribution(dataset_folder):\n",
    "    json_files = [f for f in os.listdir(dataset_folder) if f.endswith('.json')]\n",
    "    \n",
    "    dataframes = []\n",
    "    \n",
    "    for filename in json_files:\n",
    "        filepath = os.path.join(dataset_folder, filename)\n",
    "        \n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if 200 <= len(data) <= 1000:\n",
    "            df = pd.DataFrame(data)\n",
    "            dataframes.append(df)\n",
    "        elif len(data) > 1000:\n",
    "            df = pd.DataFrame(data).sample(n=1000, random_state=42)\n",
    "            dataframes.append(df)\n",
    "    \n",
    "    # Combine dataframes\n",
    "    if dataframes:\n",
    "        final_dataframe = pd.concat(dataframes, ignore_index=True)\n",
    "        \n",
    "        # Count label distribution\n",
    "        label_counts = final_dataframe['label'].value_counts()\n",
    "        \n",
    "        print(\"Label Distribution:\")\n",
    "        for label, count in label_counts.items():\n",
    "            print(f\"{label}: {count}\")\n",
    "        \n",
    "        return final_dataframe\n",
    "    else:\n",
    "        print(\"No files met the criteria.\")\n",
    "        return None\n",
    "\n",
    "# Run the analysis\n",
    "dataset_folder = 'Dataset'\n",
    "data = analyze_label_distribution(dataset_folder)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T17:58:37.626423Z",
     "start_time": "2025-02-03T17:58:37.290459Z"
    }
   },
   "id": "4f053a001f19b23e"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def get_features_1(train_text, validation_text, num_features):\n",
    "    tfidf = TfidfVectorizer(max_features=num_features)\n",
    "\n",
    "    train_text_tfidf = tfidf.fit_transform(train_text)\n",
    "\n",
    "    validation_text_tfidf = tfidf.transform(validation_text)\n",
    "\n",
    "    return train_text_tfidf.toarray(), validation_text_tfidf.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T17:58:37.626831Z",
     "start_time": "2025-02-03T17:58:37.623818Z"
    }
   },
   "id": "5f9f92824568c261"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ro_core_news_sm\")\n",
    "\n",
    "def preprocess(text):\n",
    "    text = no_diacritics(text, romanian_prefixes)\n",
    "    text = ' '.join(word for word in text.split() if word.lower() not in stop_words)\n",
    "    doc = nlp(text)\n",
    "    text = ' '.join(token.lemma_ for token in doc if not token.is_stop)\n",
    "    \n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T17:58:37.870372Z",
     "start_time": "2025-02-03T17:58:37.627336Z"
    }
   },
   "id": "ee90ddb829549a79"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0               UPDATE: Ars ca o torță la Caransebeș   \n",
      "1  OPINIA PERSONALA ,VERSUS …Raed Arafat: Trebuie...   \n",
      "2  BILANT COVID-19!5.837 cazuri noi de infectare ...   \n",
      "3  Doi bărbați au fost arestați după ce unul dint...   \n",
      "4  Bilant COVID -19 la nivelul Judetului Caras-Se...   \n",
      "\n",
      "                                                text  label content  \n",
      "0  Crimă prin lovire și incendiere de cadavru pet...  Banat     NaN  \n",
      "1  Şeful Departamentului pentru Situaţii de Urgen...  Banat     NaN  \n",
      "2  Comunica ,Grupul de Comunicare Strategică.”Pân...  Banat     NaN  \n",
      "3  Polițiștii au intervenit, miercuri seara, pent...  Banat     NaN  \n",
      "4  Pentru data de 27.11.2020 ora 8.30 vă comunică...  Banat     NaN  \n"
     ]
    },
    {
     "data": {
      "text/plain": "396      primar comună Birna , Ovidiu Ignaton , implica...\n3098     sarbatore iarnă reprezint perioadă romină , pl...\n9322     viată prieten ? fi spune prieten bun pretuiest...\n357      lună anunt Primariei timisoara potrivit fost s...\n1323     conducere natională partid popular delimitează...\n                               ...                        \n9313     fost palat comisie european Dunarii , actual s...\n10431    moment asteptat profesor parinti sosit . inspe...\n3068     anunt istoric Volkswagen - producator masin Ge...\n6104        „ mijloc bun apară < < fac rau > > : seman . ”\n3785     turn Blackpool luat foc , filmare trecatore ve...\nName: text, Length: 2399, dtype: object"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.head())\n",
    "\n",
    "data = data[data['text'].notna() | data['content'].notna()]\n",
    "\n",
    "data['text'] = data['text'].combine_first(data['content'])\n",
    "\n",
    "texts = data['text']\n",
    "labels = data['label']\n",
    "\n",
    "train_texts, validation_texts, train_labels, validation_labels = train_test_split(texts,labels, test_size=0.2, random_state=42)\n",
    "\n",
    "train_texts.apply(preprocess)\n",
    "validation_texts.apply(preprocess)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T18:06:55.288956Z",
     "start_time": "2025-02-03T17:58:37.871404Z"
    }
   },
   "id": "7abfb665b4c69d9b"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Testing for num_features = 500, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.669445602334306\n",
      "F1: 0.6714777129464439\n",
      "==================================================\n",
      "Testing for num_features = 500, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.6706961233847436\n",
      "F1: 0.674725150729272\n",
      "==================================================\n",
      "Testing for num_features = 500, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.677365568987078\n",
      "F1: 0.6814079834994059\n",
      "==================================================\n",
      "Testing for num_features = 1000, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.7590662776156732\n",
      "F1: 0.7597383193577909\n",
      "==================================================\n",
      "Testing for num_features = 1000, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.7544810337640684\n",
      "F1: 0.7550909762740887\n",
      "==================================================\n",
      "Testing for num_features = 1000, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.7561483951646519\n",
      "F1: 0.7564738383932547\n",
      "==================================================\n",
      "Testing for num_features = 1500, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.7899124635264694\n",
      "F1: 0.7911462397612014\n",
      "==================================================\n",
      "Testing for num_features = 1500, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.7919966652771988\n",
      "F1: 0.793002808786985\n",
      "==================================================\n",
      "Testing for num_features = 1500, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.7957482284285119\n",
      "F1: 0.7973997509689794\n",
      "==================================================\n",
      "Testing for num_features = 2000, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.810754481033764\n",
      "F1: 0.8104869945688848\n",
      "==================================================\n",
      "Testing for num_features = 2000, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.8145060441850771\n",
      "F1: 0.8151746280056199\n",
      "==================================================\n",
      "Testing for num_features = 2000, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.8161734055856606\n",
      "F1: 0.8169961879260181\n",
      "==================================================\n",
      "Testing for num_features = 2500, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.8207586494372655\n",
      "F1: 0.8218324284762512\n",
      "==================================================\n",
      "Testing for num_features = 2500, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.822842851187995\n",
      "F1: 0.8235513632015451\n",
      "==================================================\n",
      "Testing for num_features = 2500, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.8240933722384327\n",
      "F1: 0.8246584019810766\n"
     ]
    }
   ],
   "source": [
    "f = open(\"knn-texts-tfidf.txt\", \"w\")\n",
    "\n",
    "for num_features in [500, 1000, 1500, 2000, 2500]:\n",
    "    for num_estimators in [100, 150, 200]:\n",
    "        print(\"=\" * 50)\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "        print(f\"Testing for num_features = {num_features}, num_estimators = {num_estimators}\")\n",
    "        f.write(f\"Testing for num_features = {num_features}, num_estimators = {num_estimators}\\n\")\n",
    "\n",
    "        print(\"Creating text features...\")\n",
    "        f.write(\"Creating text features...\\n\")\n",
    "\n",
    "        train_texts_featurized, validation_texts_featurized = get_features_1(train_texts, validation_texts, num_features)\n",
    "\n",
    "        print(\"Training Random Forest...\")\n",
    "        f.write(\"Training Random Forest...\\n\")\n",
    "        random_forest = RandomForestClassifier(\n",
    "            n_estimators=num_estimators,\n",
    "            random_state=42)\n",
    "        random_forest.fit(train_texts_featurized, train_labels)\n",
    "\n",
    "        print(\"Getting predictions...\")\n",
    "        f.write(\"Getting predictions...\\n\")\n",
    "        predictions = random_forest.predict(validation_texts_featurized)\n",
    "        \n",
    "        print(\"Computing accuracy...\")\n",
    "        f.write(\"Computing accuracy...\")\n",
    "        accuracy = accuracy_score(validation_labels, predictions)\n",
    "\n",
    "        print(\"Computing f1...\")\n",
    "        f.write(\"Computing f1...\\n\")\n",
    "        f1 = f1_score(validation_labels, predictions, average='weighted')\n",
    "\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        f.write(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "        print(f\"F1: {f1}\")\n",
    "        f.write(f\"F1: {f1}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T18:09:55.255246Z",
     "start_time": "2025-02-03T18:06:55.290719Z"
    }
   },
   "id": "3a7cc62f2cd35f2c"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def get_features_2(train_text, validation_text, num_features=100, window=3, min_count=1, \n",
    "                   use_skipgram=True):\n",
    "\n",
    "    train_tokens = [text.split() for text in train_text]\n",
    "    validation_tokens = [text.split() for text in validation_text]\n",
    "    \n",
    "    # Train Word2Vec model\n",
    "    model = Word2Vec(sentences=train_tokens, \n",
    "                     vector_size=num_features, \n",
    "                     window=window, \n",
    "                     min_count=min_count,\n",
    "                     sg=1 if use_skipgram else 0)  # 1 for skip-gram, 0 for CBOW\n",
    "    \n",
    "    # Function to convert text to Word2Vec features\n",
    "    def text_to_features(tokens):\n",
    "        features = []\n",
    "        for text in tokens:\n",
    "            # Calculate mean word vector for the text\n",
    "            text_vectors = [model.wv[word] for word in text if word in model.wv]\n",
    "            \n",
    "            # If no words found, use zero vector\n",
    "            if not text_vectors:\n",
    "                features.append(np.zeros(num_features))\n",
    "            else:\n",
    "                features.append(np.mean(text_vectors, axis=0))\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    # Convert train and validation texts to features\n",
    "    train_features = text_to_features(train_tokens)\n",
    "    validation_features = text_to_features(validation_tokens)\n",
    "    \n",
    "    return train_features, validation_features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T18:09:55.260313Z",
     "start_time": "2025-02-03T18:09:55.257823Z"
    }
   },
   "id": "f8d2886a2139a232"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Testing for num_features = 500, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.554814506044185\n",
      "F1: 0.5464819506649203\n",
      "==================================================\n",
      "Testing for num_features = 500, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.5523134639433097\n",
      "F1: 0.5427678332931369\n",
      "==================================================\n",
      "Testing for num_features = 500, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.5552313463943309\n",
      "F1: 0.5474993232001415\n",
      "==================================================\n",
      "Testing for num_features = 1000, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.5523134639433097\n",
      "F1: 0.5422254017496816\n",
      "==================================================\n",
      "Testing for num_features = 1000, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.5527303042934556\n",
      "F1: 0.5451640740698598\n",
      "==================================================\n",
      "Testing for num_features = 1000, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.5573155481450605\n",
      "F1: 0.5499308368556671\n",
      "==================================================\n",
      "Testing for num_features = 1500, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.5535639849937474\n",
      "F1: 0.5451552025685261\n",
      "==================================================\n",
      "Testing for num_features = 1500, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.5498124218424344\n",
      "F1: 0.5394049352674987\n",
      "==================================================\n",
      "Testing for num_features = 1500, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.5635681533972489\n",
      "F1: 0.5564862247925552\n",
      "==================================================\n",
      "Testing for num_features = 2000, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.5498124218424344\n",
      "F1: 0.5406196248363174\n",
      "==================================================\n",
      "Testing for num_features = 2000, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.563151313047103\n",
      "F1: 0.5543722774244398\n",
      "==================================================\n",
      "Testing for num_features = 2000, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.5560650270946228\n",
      "F1: 0.544996114855485\n",
      "==================================================\n",
      "Testing for num_features = 2500, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.5439766569403919\n",
      "F1: 0.5367548284589014\n",
      "==================================================\n",
      "Testing for num_features = 2500, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.558982909545644\n",
      "F1: 0.5515713526968911\n",
      "==================================================\n",
      "Testing for num_features = 2500, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.554814506044185\n",
      "F1: 0.5468575413440021\n"
     ]
    }
   ],
   "source": [
    "f = open(\"knn-texts-word2vec.txt\", \"w\")\n",
    "\n",
    "for num_features in [500, 1000, 1500, 2000, 2500]:\n",
    "    for num_estimators in [100, 150, 200]:\n",
    "        print(\"=\" * 50)\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "        print(f\"Testing for num_features = {num_features}, num_estimators = {num_estimators}\")\n",
    "        f.write(f\"Testing for num_features = {num_features}, num_estimators = {num_estimators}\\n\")\n",
    "\n",
    "        print(\"Creating text features...\")\n",
    "        f.write(\"Creating text features...\\n\")\n",
    "\n",
    "        train_texts_featurized, validation_texts_featurized = get_features_2(train_texts, validation_texts, num_features)\n",
    "\n",
    "        print(\"Training Random Forest...\")\n",
    "        f.write(\"Training Random Forest...\\n\")\n",
    "        random_forest = RandomForestClassifier(\n",
    "            n_estimators=num_estimators,\n",
    "            random_state=42)\n",
    "        random_forest.fit(train_texts_featurized, train_labels)\n",
    "\n",
    "        print(\"Getting predictions...\")\n",
    "        f.write(\"Getting predictions...\\n\")\n",
    "        predictions = random_forest.predict(validation_texts_featurized)\n",
    "        \n",
    "        print(\"Computing accuracy...\")\n",
    "        f.write(\"Computing accuracy...\\n\")\n",
    "        accuracy = accuracy_score(validation_labels, predictions)\n",
    "\n",
    "        print(\"Computing f1...\")\n",
    "        f.write(\"Computing f1...\\n\")\n",
    "        f1 = f1_score(validation_labels, predictions, average='weighted')\n",
    "\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        f.write(f\"Accuracy: {accuracy}\\n\")\n",
    "\n",
    "        print(f\"F1: {f1}\")\n",
    "        f.write(f\"F1: {f1}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T19:08:35.206746Z",
     "start_time": "2025-02-03T18:09:55.261534Z"
    }
   },
   "id": "8f76e6bf35123dc6"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T19:08:35.216357Z",
     "start_time": "2025-02-03T19:08:35.214349Z"
    }
   },
   "id": "7de02f00d54ed3d0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
