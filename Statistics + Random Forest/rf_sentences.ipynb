{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-04T11:34:12.447085Z",
     "start_time": "2025-02-04T11:34:09.365897Z"
    }
   },
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from typing import List, Dict"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "# De aici: https://en.wiktionary.org/wiki/Category:Romanian_prefixes\n",
    "romanian_prefixes = [\n",
    "    # A\n",
    "    \"agro\", \"alt\", \"ante\", \"anti\", \"aorto\", \"arhi\", \"astro\",\n",
    "\n",
    "    # B\n",
    "    \"balano\",\n",
    "\n",
    "    # C\n",
    "    \"cardio\", \"carpo\", \"cosmo\",\n",
    "\n",
    "    # D\n",
    "    \"demono\", \"des\", \"dez\",\n",
    "\n",
    "    # F\n",
    "    \"franco\",\n",
    "\n",
    "    # G\n",
    "    \"gastro\", \"germano\", \"greco\",\n",
    "\n",
    "    # H\n",
    "    \"hecto\", \"hiper\",\n",
    "\n",
    "    # I\n",
    "    \"în\",\n",
    "\n",
    "    # K\n",
    "    \"kilo\",\n",
    "\n",
    "    # L\n",
    "    \"lexico\",\n",
    "\n",
    "    # M\n",
    "    \"mili\", \"muzico\",\n",
    "\n",
    "    # N\n",
    "    \"nano\", \"ne\",\n",
    "\n",
    "    # O\n",
    "    \"ori\", \"ornito\",\n",
    "\n",
    "    # P\n",
    "    \"pneumo\", \"pre\", \"prea\", \"proto\", \"pseudo\", \"psiho\",\n",
    "\n",
    "    # R\n",
    "    \"răs\", \"re\", \"rino\", \"ruso\",\n",
    "\n",
    "    # S\n",
    "    \"stră\", \"sub\",\n",
    "\n",
    "    # T\n",
    "    \"tehno\", \"teo\", \"termo\",\n",
    "\n",
    "    # V\n",
    "    \"vice\"\n",
    "]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T11:34:12.448046Z",
     "start_time": "2025-02-04T11:34:12.445118Z"
    }
   },
   "id": "48c4959a309f48ab",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "def replace_i_prefix(word, prefixes):\n",
    "  for prefix in prefixes:\n",
    "    try:\n",
    "      if word.lower().startswith(prefix) and len(word) > len(prefix) and word[len(prefix):][0] in [\"î\", \"Î\"]:\n",
    "        first_letter = word[len(prefix):][0]\n",
    "        first_letter = \"i\" if first_letter == \"î\" else (\"I\" if first_letter == \"Î\" else first_letter)\n",
    "        word = prefix + first_letter + word[len(prefix) + 1:]\n",
    "\n",
    "    except:\n",
    "      print(word)\n",
    "\n",
    "  word = word.replace(\"î\", \"a\").replace(\"Î\", \"A\")\n",
    "\n",
    "  return word\n",
    "\n",
    "def no_diacritics(text, prefixes):\n",
    "\n",
    "  text = replace_i_prefix(text, prefixes)\n",
    "\n",
    "\n",
    "  text = text.replace(\"â\", \"i\")\n",
    "  text = text.replace(\"Â\", \"I\")\n",
    "  text = text.replace(\"ș\", \"s\")\n",
    "  text = text.replace(\"ş\", \"s\")\n",
    "  text = text.replace(\"Ș\", \"S\")\n",
    "  text = text.replace(\"Ş\", \"S\")\n",
    "  text = text.replace(\"ț\", \"t\")\n",
    "  text = text.replace(\"ţ\", \"t\")\n",
    "  text = text.replace(\"Ț\", \"T\")\n",
    "  text = text.replace(\"Ţ\", \"T\")\n",
    "\n",
    "  # If î is the first letter of the word, replace it with i\n",
    "  if text.startswith(\"î\"):\n",
    "    text = text.replace(\"î\", \"i\")\n",
    "  if text.startswith(\"Î\"):\n",
    "    text = text.replace(\"Î\", \"I\")\n",
    "  # If the last letter of the word is î, replace it with i\n",
    "  if text.endswith(\"î\"):\n",
    "    text = text.replace(\"î\", \"i\")\n",
    "  if text.endswith(\"Î\"):\n",
    "    text = text.replace(\"Î\", \"I\")\n",
    "  # Else replace î with a\n",
    "  if \"î\" in text:\n",
    "    text = text.replace(\"î\", \"a\")\n",
    "  # text = text.replace(\"î\", \"i\")\n",
    "  # text = text.replace(\"Î\", \"I\")\n",
    "  text = text.replace(\"ă\", \"a\")\n",
    "  text = text.replace(\"Ă\", \"A\")\n",
    "\n",
    "  return text\n",
    "\n",
    "\n",
    "# for key in moldavian_texts:\n",
    "#     for i in range(len(moldavian_texts[key])):\n",
    "#         moldavian_texts[key][i] = no_diacritics(moldavian_texts[key][i], romanian_prefixes)\n",
    "\n",
    "# for key in romanian_texts:\n",
    "#     for i in range(len(romanian_texts[key])):\n",
    "#         romanian_texts[key][i] = no_diacritics(romanian_texts[key][i], romanian_prefixes)\n",
    "\n",
    "# print(moldavian_texts[\"Sport\"][0])\n",
    "# print(romanian_texts['Stiri'][12])\n",
    "\n",
    "print(no_diacritics(\"cîțiva\", romanian_prefixes))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T11:34:12.459273Z",
     "start_time": "2025-02-04T11:34:12.450155Z"
    }
   },
   "id": "892a96c01a32732f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cativa\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "romanian=[\n",
    "    \"a\", \"abia\", \"acea\", \"aceasta\", \"această\", \"aceea\", \"aceeasi\", \"acei\",\n",
    "    \"aceia\", \"acel\", \"acela\", \"acelasi\", \"acele\", \"acelea\", \"acest\", \"acesta\",\n",
    "    \"aceste\", \"acestea\", \"acestei\", \"acestia\", \"acestui\", \"aceşti\", \"aceştia\",\n",
    "    \"acești\", \"aceștia\", \"acolo\", \"acord\", \"acum\", \"adica\", \"ai\", \"aia\",\n",
    "    \"aibă\", \"aici\", \"aiurea\", \"al\", \"ala\", \"alaturi\", \"ale\", \"alea\", \"alt\",\n",
    "    \"alta\", \"altceva\", \"altcineva\", \"alte\", \"altfel\", \"alti\", \"altii\", \"altul\",\n",
    "    \"alături\", \"am\", \"anume\", \"apoi\", \"ar\", \"are\", \"as\", \"asa\", \"asemenea\",\n",
    "    \"asta\", \"astazi\", \"astea\", \"astfel\", \"astăzi\", \"asupra\", \"atare\", \"atat\",\n",
    "    \"atata\", \"atatea\", \"atatia\", \"ati\", \"atit\", \"atita\", \"atitea\", \"atitia\",\n",
    "    \"atunci\", \"au\", \"avea\", \"avem\", \"aveţi\", \"aveți\", \"avut\", \"azi\", \"aş\",\n",
    "    \"aşadar\", \"aţi\", \"aș\", \"așadar\", \"ați\", \"b\", \"ba\", \"bine\", \"bucur\", \"bună\",\n",
    "    \"c\", \"ca\", \"cam\", \"cand\", \"capat\", \"care\", \"careia\", \"carora\", \"caruia\",\n",
    "    \"cat\", \"catre\", \"caut\", \"ce\", \"cea\", \"ceea\", \"cei\", \"ceilalti\", \"cel\",\n",
    "    \"cele\", \"celor\", \"ceva\", \"chiar\", \"ci\", \"cinci\", \"cind\", \"cine\", \"cineva\",\n",
    "    \"cit\", \"cita\", \"cite\", \"citeva\", \"citi\", \"câțiva\", \"conform\", \"contra\",\n",
    "    \"cu\", \"cui\", \"cum\", \"cumva\", \"curând\", \"curînd\", \"când\", \"cât\", \"câte\",\n",
    "    \"câtva\", \"câţi\", \"câți\", \"cînd\", \"cît\", \"cîte\", \"cîtva\", \"cîţi\", \"cîți\",\n",
    "    \"că\", \"căci\", \"cărei\", \"căror\", \"cărui\", \"către\", \"d\", \"da\", \"daca\",\n",
    "    \"dacă\", \"dar\", \"dat\", \"datorită\", \"dată\", \"dau\", \"de\", \"deasupra\", \"deci\",\n",
    "    \"decit\", \"degraba\", \"deja\", \"deoarece\", \"departe\", \"desi\", \"despre\",\n",
    "    \"deşi\", \"deși\", \"din\", \"dinaintea\", \"dintr\", \"dintr-\", \"dintre\", \"doar\",\n",
    "    \"doi\", \"doilea\", \"două\", \"drept\", \"dupa\", \"după\", \"dă\", \"e\", \"ea\", \"ei\",\n",
    "    \"el\", \"ele\", \"era\", \"eram\", \"este\", \"eu\", \"exact\", \"eşti\", \"ești\", \"f\",\n",
    "    \"face\", \"fara\", \"fata\", \"fel\", \"fi\", \"fie\", \"fiecare\", \"fii\", \"fim\", \"fiu\",\n",
    "    \"fiţi\", \"fiți\", \"foarte\", \"fost\", \"frumos\", \"fără\", \"g\", \"geaba\", \"graţie\",\n",
    "    \"grație\", \"h\", \"halbă\", \"i\", \"ia\", \"iar\", \"ieri\", \"ii\", \"il\", \"imi\", \"in\",\n",
    "    \"inainte\", \"inapoi\", \"inca\", \"incit\", \"insa\", \"intr\", \"intre\", \"isi\",\n",
    "    \"iti\", \"j\", \"k\", \"l\", \"la\", \"le\", \"li\", \"lor\", \"lui\", \"lângă\", \"lîngă\",\n",
    "    \"m\", \"ma\", \"mai\", \"mare\", \"mea\", \"mei\", \"mele\", \"mereu\", \"meu\", \"mi\",\n",
    "    \"mie\", \"mine\", \"mod\", \"mult\", \"multa\", \"multe\", \"multi\", \"multă\", \"mulţi\",\n",
    "    \"mulţumesc\", \"mulți\", \"mulțumesc\", \"mâine\", \"mîine\", \"mă\", \"n\", \"ne\",\n",
    "    \"nevoie\", \"ni\", \"nici\", \"niciodata\", \"nicăieri\", \"nimeni\", \"nimeri\",\n",
    "    \"nimic\", \"niste\", \"nişte\", \"niște\", \"noastre\", \"noastră\", \"noi\", \"noroc\",\n",
    "    \"nostri\", \"nostru\", \"nou\", \"noua\", \"nouă\", \"noştri\", \"noștri\", \"nu\",\n",
    "    \"numai\", \"o\", \"opt\", \"or\", \"ori\", \"oricare\", \"orice\", \"oricine\", \"oricum\",\n",
    "    \"oricând\", \"oricât\", \"oricînd\", \"oricît\", \"oriunde\", \"p\", \"pai\", \"parca\",\n",
    "    \"patra\", \"patru\", \"patrulea\", \"pe\", \"pentru\", \"peste\", \"pic\", \"pina\",\n",
    "    \"plus\", \"poate\", \"pot\", \"prea\", \"prima\", \"primul\", \"prin\", \"printr-\",\n",
    "    \"putini\", \"puţin\", \"puţina\", \"puţină\", \"puțin\", \"puțina\", \"puțină\", \"până\",\n",
    "    \"pînă\", \"r\", \"rog\", \"s\", \"sa\", \"sa-mi\", \"sa-ti\", \"sai\", \"sale\", \"sau\",\n",
    "    \"se\", \"si\", \"sint\", \"sintem\", \"spate\", \"spre\", \"sub\", \"sunt\", \"suntem\",\n",
    "    \"sunteţi\", \"sunteți\", \"sus\", \"sută\", \"sînt\", \"sîntem\", \"sînteţi\",\n",
    "    \"sînteți\", \"să\", \"săi\", \"său\", \"t\", \"ta\", \"tale\", \"te\", \"ti\", \"timp\",\n",
    "    \"tine\", \"toata\", \"toate\", \"toată\", \"tocmai\", \"tot\", \"toti\", \"totul\",\n",
    "    \"totusi\", \"totuşi\", \"totuși\", \"toţi\", \"toți\", \"trei\", \"treia\", \"treilea\",\n",
    "    \"tu\", \"tuturor\", \"tăi\", \"tău\", \"u\", \"ul\", \"ului\", \"un\", \"una\", \"unde\",\n",
    "    \"undeva\", \"unei\", \"uneia\", \"unele\", \"uneori\", \"unii\", \"unor\", \"unora\",\n",
    "    \"unu\", \"unui\", \"unuia\", \"unul\", \"v\", \"va\", \"vi\", \"voastre\", \"voastră\",\n",
    "    \"voi\", \"vom\", \"vor\", \"vostru\", \"vouă\", \"voştri\", \"voștri\", \"vreme\", \"vreo\",\n",
    "    \"vreun\", \"vă\", \"x\", \"z\", \"zece\", \"zero\", \"zi\", \"zice\", \"îi\", \"îl\", \"îmi\",\n",
    "    \"împotriva\", \"în\", \"înainte\", \"înaintea\", \"încotro\", \"încât\", \"încît\",\n",
    "    \"între\", \"întrucât\", \"întrucît\", \"îţi\", \"îți\", \"ăla\", \"ălea\", \"ăsta\",\n",
    "    \"ăstea\", \"ăştia\", \"ăștia\", \"şapte\", \"şase\", \"şi\", \"ştiu\", \"ţi\", \"ţie\",\n",
    "    \"șapte\", \"șase\", \"și\", \"știu\", \"ți\", \"ție\"\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T11:34:12.460903Z",
     "start_time": "2025-02-04T11:34:12.457940Z"
    }
   },
   "id": "6f424384dbf01e6b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "# # Get all the words from the stop words list and apply the same transformation\n",
    "stop_words = romanian\n",
    "for i in range(len(stop_words)):\n",
    "    stop_words[i] = no_diacritics(stop_words[i], romanian_prefixes)\n",
    "\n",
    "stop_words = list(set(stop_words))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T11:34:12.471352Z",
     "start_time": "2025-02-04T11:34:12.462180Z"
    }
   },
   "id": "a152024aa25a71cb",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "def analyze_label_distribution(dataset_folder: str) -> pd.DataFrame:\n",
    "    json_files = [f for f in os.listdir(dataset_folder) if f.endswith('.json')]\n",
    "    \n",
    "    label_data: Dict[str, List[Dict]] = {}\n",
    "    \n",
    "    for filename in json_files:\n",
    "        filepath = os.path.join(dataset_folder, filename)\n",
    "        \n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                \n",
    "                if isinstance(data, dict):\n",
    "                    df = pd.DataFrame.from_dict(data, orient='index')\n",
    "                else:\n",
    "                    df = pd.DataFrame(data)\n",
    "                \n",
    "                for _, row in df.iterrows():\n",
    "                    \n",
    "                    try:\n",
    "                        label = row.get('label', row.get('category', row.get('class')))\n",
    "                        text = row.get('text', row.get('content', row.get('description')))\n",
    "                        \n",
    "                        if label is not None and text is not None:\n",
    "                            if label not in label_data:\n",
    "                                label_data[label] = []\n",
    "                            label_data[label].append({'text': text, 'label': label})\n",
    "                    except AttributeError:\n",
    "                        print(f\"Skipping row in {filename} due to unexpected format\")\n",
    "                        continue\n",
    "                    \n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error reading {filename}: Invalid JSON format\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    if not label_data:\n",
    "        print(\"No valid data found in any of the JSON files.\")\n",
    "        return None\n",
    "    \n",
    "    label_data = {k: v for k, v in label_data.items() if len(v) >= 200}\n",
    "    \n",
    "    if not label_data:\n",
    "        print(\"No labels met the minimum criteria of 200 examples.\")\n",
    "        return None\n",
    "    \n",
    "    processed_data = []\n",
    "    \n",
    "    for label, texts in label_data.items():\n",
    "        \n",
    "        all_sentences = []\n",
    "        for item in texts:\n",
    "            sentences = sent_tokenize(item['text'])\n",
    "            all_sentences.extend([(sentence, item['label']) for sentence in sentences])\n",
    "        \n",
    "        if len(all_sentences) > 10000:\n",
    "            selected_indices = np.random.choice(len(all_sentences), 10000, replace=False)\n",
    "            selected_sentences = [all_sentences[i] for i in selected_indices]\n",
    "        else:\n",
    "            selected_sentences = all_sentences\n",
    "        \n",
    "        for sentence, label in selected_sentences:\n",
    "            processed_data.append({\n",
    "                'text': sentence,\n",
    "                'label': label\n",
    "            })\n",
    "\n",
    "    final_dataframe = pd.DataFrame(processed_data)\n",
    "\n",
    "    label_counts = final_dataframe['label'].value_counts()\n",
    "    print(\"\\nFinal Label Distribution:\")\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"{label}: {count} sentences\")\n",
    "    \n",
    "    return final_dataframe\n",
    "\n",
    "dataset_folder = 'Dataset'\n",
    "data = analyze_label_distribution(dataset_folder)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T11:34:17.370677Z",
     "start_time": "2025-02-04T11:34:12.468896Z"
    }
   },
   "id": "4f053a001f19b23e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Label Distribution:\n",
      "Banat: 10000 sentences\n",
      "Criuleni: 10000 sentences\n",
      "Oltenia: 10000 sentences\n",
      "Moldova: 10000 sentences\n",
      "Serbia: 10000 sentences\n",
      "Muntenia: 10000 sentences\n",
      "Ungheni: 10000 sentences\n",
      "Dobrogea: 10000 sentences\n",
      "Germania: 10000 sentences\n",
      "Ardeal: 10000 sentences\n",
      "Sangerei: 10000 sentences\n",
      "Ucraina: 10000 sentences\n",
      "Canada_EN: 9779 sentences\n",
      "Calarasi: 8187 sentences\n",
      "Spania: 8014 sentences\n",
      "Crisana: 7837 sentences\n",
      "Bucovina: 7031 sentences\n",
      "Maramures: 7019 sentences\n",
      "UK: 7008 sentences\n",
      "Soroca: 6548 sentences\n",
      "Ialoveni: 5931 sentences\n",
      "Orhei: 5064 sentences\n",
      "Cahul: 4114 sentences\n",
      "Causeni: 3254 sentences\n",
      "Balti: 1377 sentences\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "def get_features_1(train_text, validation_text, num_features):\n",
    "    tfidf = TfidfVectorizer(max_features=num_features)\n",
    "\n",
    "    train_text_tfidf = tfidf.fit_transform(train_text)\n",
    "\n",
    "    validation_text_tfidf = tfidf.transform(validation_text)\n",
    "\n",
    "    return train_text_tfidf.toarray(), validation_text_tfidf.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T11:34:17.375434Z",
     "start_time": "2025-02-04T11:34:17.371582Z"
    }
   },
   "id": "5f9f92824568c261",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "nlp = spacy.load(\"ro_core_news_sm\")\n",
    "\n",
    "def preprocess(text):\n",
    "    text = no_diacritics(text, romanian_prefixes)\n",
    "    text = ' '.join(word for word in text.split() if word.lower() in stop_words)\n",
    "    doc = nlp(text)\n",
    "    text = ' '.join(token.lemma_ for token in doc if not token.is_stop)\n",
    "    \n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T11:34:17.584853Z",
     "start_time": "2025-02-04T11:34:17.374574Z"
    }
   },
   "id": "ee90ddb829549a79",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "print(data.head())\n",
    "\n",
    "data = data[data['text'].notna()]\n",
    "\n",
    "texts = data['text']\n",
    "labels = data['label']\n",
    "\n",
    "train_texts, validation_texts, train_labels, validation_labels = train_test_split(texts,labels, test_size=0.2, random_state=42)\n",
    "\n",
    "train_texts.apply(preprocess)\n",
    "validation_texts.apply(preprocess)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T11:45:24.801410Z",
     "start_time": "2025-02-04T11:34:17.585728Z"
    }
   },
   "id": "7abfb665b4c69d9b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  De asemenea, 74.922 de persoane se află în car...  Banat\n",
      "1  EEI, firma noastră locală, are și ea un merit ...  Banat\n",
      "2  Vorbim despre tinere care, adeseori, sunt resp...  Banat\n",
      "3  Încât, pentru mamă, s-a constatat cum că ea se...  Banat\n",
      "4  „Dispune eliminarea fizică din dosarul de urmă...  Banat\n"
     ]
    },
    {
     "data": {
      "text/plain": "28113          \n73653        an\n158800         \n194447    an an\n43245     an an\n          ...  \n44446     an an\n57363          \n90483          \n109754         \n60052        an\nName: text, Length: 40233, dtype: object"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T11:45:24.938318Z",
     "start_time": "2025-02-04T11:45:24.816439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_texts = train_texts[train_texts.str.strip() != '']\n",
    "train_texts = train_texts[train_texts.notna()]\n",
    "validation_texts = validation_texts[validation_texts.str.strip() != '']\n",
    "validation_texts = validation_texts[validation_texts.notna()]\n",
    "\n",
    "train_labels = train_labels[train_texts.index]\n",
    "train_labels = train_labels[train_labels.notna()]\n",
    "validation_labels = validation_labels[validation_texts.index]\n",
    "validation_labels = validation_labels[validation_labels.notna()]\n",
    "\n",
    "final_train_indices = train_texts.index.intersection(train_labels.index)\n",
    "final_val_indices = validation_texts.index.intersection(validation_labels.index)\n",
    "\n",
    "train_texts = train_texts[final_train_indices]\n",
    "train_labels = train_labels[final_train_indices]\n",
    "validation_texts = validation_texts[final_val_indices]\n",
    "validation_labels = validation_labels[final_val_indices]\n",
    "\n",
    "print(\"\\nFinal dataset sizes after cleaning:\")\n",
    "print(f\"Final training samples: {len(train_texts)}\")\n",
    "print(f\"Final validation samples: {len(validation_texts)}\")"
   ],
   "id": "df9e5a7de2df5cb0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final dataset sizes after cleaning:\n",
      "Final training samples: 160930\n",
      "Final validation samples: 40233\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T11:45:24.943423Z",
     "start_time": "2025-02-04T11:45:24.919418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(train_texts[1:20])\n",
    "print(validation_texts[1:20])\n",
    "\n",
    "print(train_labels[1:20])\n",
    "print(validation_labels[1:20])"
   ],
   "id": "425e10b4121aed8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165076    In timpul spectacolului, mama sa, modelul Maye...\n",
      "6854      La pagina 16 a cărții găsim un tabel comparati...\n",
      "49895         Adăugați toate ingredientele într-un blender.\n",
      "189517    Legătura dintre stres și anxietate este una pr...\n",
      "188662    Consiliul Local al comunei Bâlteni a aprobat o...\n",
      "66257     În același timp, Sainsbury’s și Tesco s-au reg...\n",
      "158341    Femeia, care este pensionară, le-a povestit oa...\n",
      "134077             La eveniment au participat și invitaţii.\n",
      "191056    După ce a câștigat alegerile, Dijmărescu a fos...\n",
      "133134    Conform celor menționate de dânsul, Comunitate...\n",
      "125895    Deși a îndrăgit foarte mult orașul Timișoara, ...\n",
      "4878      Petru, ducându-se la cârciuma lui Ioniţă Georg...\n",
      "43647     Azi pe multe dintre tractoarele prezente la ma...\n",
      "64910     În Scoția, Yorkshire, Lancashire și Staffordsh...\n",
      "170803    Un alt obiectiv ce mi-l propun este identifica...\n",
      "100999    În data de 20.01.2025, o echipă comună formată...\n",
      "73382     De notat că, deocamdată nici plaje amenajate c...\n",
      "36040     9863/13.06.2024 (1.300.177,34 lei), respectiv ...\n",
      "181466    Judecătoria Tg-Jiu a dispus, la finele săptămâ...\n",
      "Name: text, dtype: object\n",
      "73653     – Cine este responsabil acum în teritoriu de o...\n",
      "158800    Natura este deosebit de frumoasă, iar atmosfer...\n",
      "194447    În zona de frontieră de pe malul românesc al D...\n",
      "43245     Dar în scurta sa carieră în vamă de până acum,...\n",
      "107558    Aveam idee despre cine e vorba, dar nu aveam t...\n",
      "177496    Mulți dintre ei consideră că dansul este o inv...\n",
      "138789    Unul dintre cele mai delicate momente ale cari...\n",
      "61224     Incendiul a fost lichidat, victime nu au fost ...\n",
      "17436     „ Dorim să mulţumim pe această cale domnului G...\n",
      "69818     Nu mai există niciun alt suspect care ar fi im...\n",
      "65983     Plafonul de preț al energiei acoperă 26 de mil...\n",
      "153070    Aici vorbim despre susținerea părinților cu co...\n",
      "8105                         Acum a desemnat un câștigător.\n",
      "170872    „La fiecare sesiune, oferim atât opțiuni contr...\n",
      "94736     Totodată, s-au finalizat lucrările esențiale l...\n",
      "151324    Nu este nimic secret!” Pentru canadieni, decla...\n",
      "142104    Domnișoare de onoare la nuntă în Băiuț – Sursă...\n",
      "50162     „Carte Verde” este un document internațional c...\n",
      "176859    Şi iată că în 1972, fiind deja cunoscut în cer...\n",
      "Name: text, dtype: object\n",
      "165076     Moldova\n",
      "6854         Banat\n",
      "49895      Causeni\n",
      "189517     Oltenia\n",
      "188662     Oltenia\n",
      "66257           UK\n",
      "158341    Calarasi\n",
      "134077      Serbia\n",
      "191056     Oltenia\n",
      "133134      Serbia\n",
      "125895      Serbia\n",
      "4878         Banat\n",
      "43647     Germania\n",
      "64910           UK\n",
      "170803       Cahul\n",
      "100999    Dobrogea\n",
      "73382     Criuleni\n",
      "36040       Ardeal\n",
      "181466     Oltenia\n",
      "Name: label, dtype: object\n",
      "73653      Criuleni\n",
      "158800     Calarasi\n",
      "194447      Ucraina\n",
      "43245      Germania\n",
      "107558      Crisana\n",
      "177496       Soroca\n",
      "138789    Maramures\n",
      "61224         Orhei\n",
      "17436        Spania\n",
      "69818            UK\n",
      "65983            UK\n",
      "153070     Calarasi\n",
      "8105          Banat\n",
      "170872        Cahul\n",
      "94736      Dobrogea\n",
      "151324    Canada_EN\n",
      "142104    Maramures\n",
      "50162       Causeni\n",
      "176859       Soroca\n",
      "Name: label, dtype: object\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "f = open(\"rf-sentences-tfidf.txt\", \"w\")\n",
    "\n",
    "for num_features in [500, 1000, 1500, 2000, 2500]:\n",
    "    for num_estimators in [100, 150, 200]:\n",
    "        print(\"=\" * 50)\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "        print(f\"Testing for num_features = {num_features}, num_estimators = {num_estimators}\")\n",
    "        f.write(f\"Testing for num_features = {num_features}, num_estimators = {num_estimators}\\n\")\n",
    "\n",
    "        print(\"Creating text features...\")\n",
    "        f.write(\"Creating text features...\\n\")\n",
    "\n",
    "        train_texts_featurized, validation_texts_featurized = get_features_1(train_texts, validation_texts, num_features)\n",
    "\n",
    "        print(\"Training Random Forest...\")\n",
    "        f.write(\"Training Random Forest...\\n\")\n",
    "        random_forest = RandomForestClassifier(\n",
    "            n_estimators=num_estimators,\n",
    "            random_state=42)\n",
    "        random_forest.fit(train_texts_featurized, train_labels)\n",
    "\n",
    "        print(\"Getting predictions...\")\n",
    "        f.write(\"Getting predictions...\\n\")\n",
    "        predictions = random_forest.predict(validation_texts_featurized)\n",
    "        \n",
    "        print(\"Computing accuracy...\")\n",
    "        f.write(\"Computing accuracy...\")\n",
    "        accuracy = accuracy_score(validation_labels, predictions)\n",
    "\n",
    "        print(\"Computing f1...\")\n",
    "        f.write(\"Computing f1...\\n\")\n",
    "        f1 = f1_score(validation_labels, predictions, average='weighted')\n",
    "\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        f.write(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "        print(f\"F1: {f1}\")\n",
    "        f.write(f\"F1: {f1}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T13:35:12.472795Z",
     "start_time": "2025-02-04T13:35:12.469149Z"
    }
   },
   "id": "3a7cc62f2cd35f2c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Testing for num_features = 500, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.3381303904754803\n",
      "F1: 0.337371901480755\n",
      "==================================================\n",
      "Testing for num_features = 500, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.3411627271145577\n",
      "F1: 0.33987721353865197\n",
      "==================================================\n",
      "Testing for num_features = 500, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.3426540402157433\n",
      "F1: 0.3407174981508568\n",
      "==================================================\n",
      "Testing for num_features = 1000, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.376432281957597\n",
      "F1: 0.37523337948002033\n",
      "==================================================\n",
      "Testing for num_features = 1000, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.3793900529416151\n",
      "F1: 0.3776553198355999\n",
      "==================================================\n",
      "Testing for num_features = 1000, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.38406283399199664\n",
      "F1: 0.38192953189384565\n",
      "==================================================\n",
      "Testing for num_features = 1500, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.40076554072527526\n",
      "F1: 0.40019378752842083\n",
      "==================================================\n",
      "Testing for num_features = 1500, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.40243084035493254\n",
      "F1: 0.4012447898076354\n",
      "==================================================\n",
      "Testing for num_features = 1500, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.4043695473864738\n",
      "F1: 0.402865427548124\n",
      "==================================================\n",
      "Testing for num_features = 2000, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.4131185842467626\n",
      "F1: 0.41284211030878476\n",
      "==================================================\n",
      "Testing for num_features = 2000, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.4169711430914921\n",
      "F1: 0.4159808883059256\n",
      "==================================================\n",
      "Testing for num_features = 2000, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.4198652153264902\n",
      "F1: 0.41890346024979266\n",
      "==================================================\n",
      "Testing for num_features = 2500, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.4242567278355098\n",
      "F1: 0.42313201937251013\n",
      "==================================================\n",
      "Testing for num_features = 2500, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.4281092866802393\n",
      "F1: 0.4265937243066196\n",
      "==================================================\n",
      "Testing for num_features = 2500, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.4319618455249688\n",
      "F1: 0.4307083302413291\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": [
    "def get_features_2(train_text, validation_text, num_features=100, window=3, min_count=1, \n",
    "                   use_skipgram=True):\n",
    "\n",
    "    train_tokens = [text.split() for text in train_text]\n",
    "    validation_tokens = [text.split() for text in validation_text]\n",
    "    \n",
    "    model = Word2Vec(sentences=train_tokens, \n",
    "                     vector_size=num_features, \n",
    "                     window=window, \n",
    "                     min_count=min_count,\n",
    "                     sg=1 if use_skipgram else 0) \n",
    "    \n",
    "    def text_to_features(tokens):\n",
    "        features = []\n",
    "        for text in tokens:\n",
    "            text_vectors = [model.wv[word] for word in text if word in model.wv]\n",
    "            \n",
    "            if not text_vectors:\n",
    "                features.append(np.zeros(num_features))\n",
    "            else:\n",
    "                features.append(np.mean(text_vectors, axis=0))\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    train_features = text_to_features(train_tokens)\n",
    "    validation_features = text_to_features(validation_tokens)\n",
    "    \n",
    "    return train_features, validation_features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T13:39:11.273100Z",
     "start_time": "2025-02-04T13:39:11.268141Z"
    }
   },
   "id": "f8d2886a2139a232",
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": [
    "f = open(\"rf-sentences-word2vec.txt\", \"w\")\n",
    "\n",
    "for num_features in [500, 1000, 1500, 2000, 2500]:\n",
    "    for num_estimators in [100, 150, 200]:\n",
    "        print(\"=\" * 50)\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "        print(f\"Testing for num_features = {num_features}, num_estimators = {num_estimators}\")\n",
    "        f.write(f\"Testing for num_features = {num_features}, num_estimators = {num_estimators}\\n\")\n",
    "\n",
    "        print(\"Creating text features...\")\n",
    "        f.write(\"Creating text features...\\n\")\n",
    "\n",
    "        train_texts_featurized, validation_texts_featurized = get_features_2(train_texts, validation_texts, num_features)\n",
    "\n",
    "        print(\"Training Random Forest...\")\n",
    "        f.write(\"Training Random Forest...\\n\")\n",
    "        random_forest = RandomForestClassifier(\n",
    "            n_estimators=num_estimators,\n",
    "            random_state=42)\n",
    "        random_forest.fit(train_texts_featurized, train_labels)\n",
    "\n",
    "        print(\"Getting predictions...\")\n",
    "        f.write(\"Getting predictions...\\n\")\n",
    "        predictions = random_forest.predict(validation_texts_featurized)\n",
    "        \n",
    "        print(\"Computing accuracy...\")\n",
    "        f.write(\"Computing accuracy...\\n\")\n",
    "        accuracy = accuracy_score(validation_labels, predictions)\n",
    "\n",
    "        print(\"Computing f1...\")\n",
    "        f.write(\"Computing f1...\\n\")\n",
    "        f1 = f1_score(validation_labels, predictions, average='weighted')\n",
    "\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        f.write(f\"Accuracy: {accuracy}\\n\")\n",
    "\n",
    "        print(f\"F1: {f1}\")\n",
    "        f.write(f\"F1: {f1}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T13:51:55.006339Z",
     "start_time": "2025-02-04T13:51:54.993162Z"
    }
   },
   "id": "8f76e6bf35123dc6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Testing for num_features = 500, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.27050431238038424\n",
      "F1: 0.269897521184604\n",
      "==================================================\n",
      "Testing for num_features = 500, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.27293018169164616\n",
      "F1: 0.271902170830922\n",
      "==================================================\n",
      "Testing for num_features = 500, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.27412323217259464\n",
      "F1: 0.27257399852068544\n",
      "==================================================\n",
      "Testing for num_features = 1000, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.3011458255660776\n",
      "F1: 0.3001867035840163\n",
      "==================================================\n",
      "Testing for num_features = 1000, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.303512042353292\n",
      "F1: 0.3021242558684799\n",
      "==================================================\n",
      "Testing for num_features = 1000, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.3072502671935973\n",
      "F1: 0.3055436255550765\n",
      "==================================================\n",
      "Testing for num_features = 1500, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.3206124325802202\n",
      "F1: 0.32015502802273666\n",
      "==================================================\n",
      "Testing for num_features = 1500, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.3219446722839461\n",
      "F1: 0.3209958318461083\n",
      "==================================================\n",
      "Testing for num_features = 1500, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.32349163790917904\n",
      "F1: 0.3226923420385012\n",
      "==================================================\n",
      "Testing for num_features = 2000, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.33049486739741\n",
      "F1: 0.3302737681670278\n",
      "==================================================\n",
      "Testing for num_features = 2000, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.33357691447319365\n",
      "F1: 0.3327766706447405\n",
      "==================================================\n",
      "Testing for num_features = 2000, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.3358921722611922\n",
      "F1: 0.3351227681997941\n",
      "==================================================\n",
      "Testing for num_features = 2500, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.33940538226840786\n",
      "F1: 0.3385056155380081\n",
      "==================================================\n",
      "Testing for num_features = 2500, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.34248743734419144\n",
      "F1: 0.3412749794452957\n",
      "==================================================\n",
      "Testing for num_features = 2500, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.3455768764199751\n",
      "F1: 0.3446570641930633\n"
     ]
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
