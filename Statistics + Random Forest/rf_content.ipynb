{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-04T09:55:35.584339Z",
     "start_time": "2025-02-04T09:55:35.581015Z"
    }
   },
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from typing import List, Dict"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "# De aici: https://en.wiktionary.org/wiki/Category:Romanian_prefixes\n",
    "romanian_prefixes = [\n",
    "    # A\n",
    "    \"agro\", \"alt\", \"ante\", \"anti\", \"aorto\", \"arhi\", \"astro\",\n",
    "\n",
    "    # B\n",
    "    \"balano\",\n",
    "\n",
    "    # C\n",
    "    \"cardio\", \"carpo\", \"cosmo\",\n",
    "\n",
    "    # D\n",
    "    \"demono\", \"des\", \"dez\",\n",
    "\n",
    "    # F\n",
    "    \"franco\",\n",
    "\n",
    "    # G\n",
    "    \"gastro\", \"germano\", \"greco\",\n",
    "\n",
    "    # H\n",
    "    \"hecto\", \"hiper\",\n",
    "\n",
    "    # I\n",
    "    \"în\",\n",
    "\n",
    "    # K\n",
    "    \"kilo\",\n",
    "\n",
    "    # L\n",
    "    \"lexico\",\n",
    "\n",
    "    # M\n",
    "    \"mili\", \"muzico\",\n",
    "\n",
    "    # N\n",
    "    \"nano\", \"ne\",\n",
    "\n",
    "    # O\n",
    "    \"ori\", \"ornito\",\n",
    "\n",
    "    # P\n",
    "    \"pneumo\", \"pre\", \"prea\", \"proto\", \"pseudo\", \"psiho\",\n",
    "\n",
    "    # R\n",
    "    \"răs\", \"re\", \"rino\", \"ruso\",\n",
    "\n",
    "    # S\n",
    "    \"stră\", \"sub\",\n",
    "\n",
    "    # T\n",
    "    \"tehno\", \"teo\", \"termo\",\n",
    "\n",
    "    # V\n",
    "    \"vice\"\n",
    "]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T09:55:36.002094Z",
     "start_time": "2025-02-04T09:55:36.000128Z"
    }
   },
   "id": "48c4959a309f48ab",
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "def replace_i_prefix(word, prefixes):\n",
    "  for prefix in prefixes:\n",
    "    try:\n",
    "      if word.lower().startswith(prefix) and len(word) > len(prefix) and word[len(prefix):][0] in [\"î\", \"Î\"]:\n",
    "        first_letter = word[len(prefix):][0]\n",
    "        first_letter = \"i\" if first_letter == \"î\" else (\"I\" if first_letter == \"Î\" else first_letter)\n",
    "        word = prefix + first_letter + word[len(prefix) + 1:]\n",
    "\n",
    "    except:\n",
    "      print(word)\n",
    "\n",
    "  word = word.replace(\"î\", \"a\").replace(\"Î\", \"A\")\n",
    "\n",
    "  return word\n",
    "\n",
    "def no_diacritics(text, prefixes):\n",
    "\n",
    "  text = replace_i_prefix(text, prefixes)\n",
    "\n",
    "\n",
    "  text = text.replace(\"â\", \"i\")\n",
    "  text = text.replace(\"Â\", \"I\")\n",
    "  text = text.replace(\"ș\", \"s\")\n",
    "  text = text.replace(\"ş\", \"s\")\n",
    "  text = text.replace(\"Ș\", \"S\")\n",
    "  text = text.replace(\"Ş\", \"S\")\n",
    "  text = text.replace(\"ț\", \"t\")\n",
    "  text = text.replace(\"ţ\", \"t\")\n",
    "  text = text.replace(\"Ț\", \"T\")\n",
    "  text = text.replace(\"Ţ\", \"T\")\n",
    "\n",
    "  # If î is the first letter of the word, replace it with i\n",
    "  if text.startswith(\"î\"):\n",
    "    text = text.replace(\"î\", \"i\")\n",
    "  if text.startswith(\"Î\"):\n",
    "    text = text.replace(\"Î\", \"I\")\n",
    "  # If the last letter of the word is î, replace it with i\n",
    "  if text.endswith(\"î\"):\n",
    "    text = text.replace(\"î\", \"i\")\n",
    "  if text.endswith(\"Î\"):\n",
    "    text = text.replace(\"Î\", \"I\")\n",
    "  # Else replace î with a\n",
    "  if \"î\" in text:\n",
    "    text = text.replace(\"î\", \"a\")\n",
    "  # text = text.replace(\"î\", \"i\")\n",
    "  # text = text.replace(\"Î\", \"I\")\n",
    "  text = text.replace(\"ă\", \"a\")\n",
    "  text = text.replace(\"Ă\", \"A\")\n",
    "\n",
    "  return text\n",
    "\n",
    "\n",
    "# for key in moldavian_texts:\n",
    "#     for i in range(len(moldavian_texts[key])):\n",
    "#         moldavian_texts[key][i] = no_diacritics(moldavian_texts[key][i], romanian_prefixes)\n",
    "\n",
    "# for key in romanian_texts:\n",
    "#     for i in range(len(romanian_texts[key])):\n",
    "#         romanian_texts[key][i] = no_diacritics(romanian_texts[key][i], romanian_prefixes)\n",
    "\n",
    "# print(moldavian_texts[\"Sport\"][0])\n",
    "# print(romanian_texts['Stiri'][12])\n",
    "\n",
    "print(no_diacritics(\"cîțiva\", romanian_prefixes))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T09:55:37.004432Z",
     "start_time": "2025-02-04T09:55:37.002289Z"
    }
   },
   "id": "892a96c01a32732f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cativa\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": [
    "romanian=[\n",
    "    \"a\", \"abia\", \"acea\", \"aceasta\", \"această\", \"aceea\", \"aceeasi\", \"acei\",\n",
    "    \"aceia\", \"acel\", \"acela\", \"acelasi\", \"acele\", \"acelea\", \"acest\", \"acesta\",\n",
    "    \"aceste\", \"acestea\", \"acestei\", \"acestia\", \"acestui\", \"aceşti\", \"aceştia\",\n",
    "    \"acești\", \"aceștia\", \"acolo\", \"acord\", \"acum\", \"adica\", \"ai\", \"aia\",\n",
    "    \"aibă\", \"aici\", \"aiurea\", \"al\", \"ala\", \"alaturi\", \"ale\", \"alea\", \"alt\",\n",
    "    \"alta\", \"altceva\", \"altcineva\", \"alte\", \"altfel\", \"alti\", \"altii\", \"altul\",\n",
    "    \"alături\", \"am\", \"anume\", \"apoi\", \"ar\", \"are\", \"as\", \"asa\", \"asemenea\",\n",
    "    \"asta\", \"astazi\", \"astea\", \"astfel\", \"astăzi\", \"asupra\", \"atare\", \"atat\",\n",
    "    \"atata\", \"atatea\", \"atatia\", \"ati\", \"atit\", \"atita\", \"atitea\", \"atitia\",\n",
    "    \"atunci\", \"au\", \"avea\", \"avem\", \"aveţi\", \"aveți\", \"avut\", \"azi\", \"aş\",\n",
    "    \"aşadar\", \"aţi\", \"aș\", \"așadar\", \"ați\", \"b\", \"ba\", \"bine\", \"bucur\", \"bună\",\n",
    "    \"c\", \"ca\", \"cam\", \"cand\", \"capat\", \"care\", \"careia\", \"carora\", \"caruia\",\n",
    "    \"cat\", \"catre\", \"caut\", \"ce\", \"cea\", \"ceea\", \"cei\", \"ceilalti\", \"cel\",\n",
    "    \"cele\", \"celor\", \"ceva\", \"chiar\", \"ci\", \"cinci\", \"cind\", \"cine\", \"cineva\",\n",
    "    \"cit\", \"cita\", \"cite\", \"citeva\", \"citi\", \"câțiva\", \"conform\", \"contra\",\n",
    "    \"cu\", \"cui\", \"cum\", \"cumva\", \"curând\", \"curînd\", \"când\", \"cât\", \"câte\",\n",
    "    \"câtva\", \"câţi\", \"câți\", \"cînd\", \"cît\", \"cîte\", \"cîtva\", \"cîţi\", \"cîți\",\n",
    "    \"că\", \"căci\", \"cărei\", \"căror\", \"cărui\", \"către\", \"d\", \"da\", \"daca\",\n",
    "    \"dacă\", \"dar\", \"dat\", \"datorită\", \"dată\", \"dau\", \"de\", \"deasupra\", \"deci\",\n",
    "    \"decit\", \"degraba\", \"deja\", \"deoarece\", \"departe\", \"desi\", \"despre\",\n",
    "    \"deşi\", \"deși\", \"din\", \"dinaintea\", \"dintr\", \"dintr-\", \"dintre\", \"doar\",\n",
    "    \"doi\", \"doilea\", \"două\", \"drept\", \"dupa\", \"după\", \"dă\", \"e\", \"ea\", \"ei\",\n",
    "    \"el\", \"ele\", \"era\", \"eram\", \"este\", \"eu\", \"exact\", \"eşti\", \"ești\", \"f\",\n",
    "    \"face\", \"fara\", \"fata\", \"fel\", \"fi\", \"fie\", \"fiecare\", \"fii\", \"fim\", \"fiu\",\n",
    "    \"fiţi\", \"fiți\", \"foarte\", \"fost\", \"frumos\", \"fără\", \"g\", \"geaba\", \"graţie\",\n",
    "    \"grație\", \"h\", \"halbă\", \"i\", \"ia\", \"iar\", \"ieri\", \"ii\", \"il\", \"imi\", \"in\",\n",
    "    \"inainte\", \"inapoi\", \"inca\", \"incit\", \"insa\", \"intr\", \"intre\", \"isi\",\n",
    "    \"iti\", \"j\", \"k\", \"l\", \"la\", \"le\", \"li\", \"lor\", \"lui\", \"lângă\", \"lîngă\",\n",
    "    \"m\", \"ma\", \"mai\", \"mare\", \"mea\", \"mei\", \"mele\", \"mereu\", \"meu\", \"mi\",\n",
    "    \"mie\", \"mine\", \"mod\", \"mult\", \"multa\", \"multe\", \"multi\", \"multă\", \"mulţi\",\n",
    "    \"mulţumesc\", \"mulți\", \"mulțumesc\", \"mâine\", \"mîine\", \"mă\", \"n\", \"ne\",\n",
    "    \"nevoie\", \"ni\", \"nici\", \"niciodata\", \"nicăieri\", \"nimeni\", \"nimeri\",\n",
    "    \"nimic\", \"niste\", \"nişte\", \"niște\", \"noastre\", \"noastră\", \"noi\", \"noroc\",\n",
    "    \"nostri\", \"nostru\", \"nou\", \"noua\", \"nouă\", \"noştri\", \"noștri\", \"nu\",\n",
    "    \"numai\", \"o\", \"opt\", \"or\", \"ori\", \"oricare\", \"orice\", \"oricine\", \"oricum\",\n",
    "    \"oricând\", \"oricât\", \"oricînd\", \"oricît\", \"oriunde\", \"p\", \"pai\", \"parca\",\n",
    "    \"patra\", \"patru\", \"patrulea\", \"pe\", \"pentru\", \"peste\", \"pic\", \"pina\",\n",
    "    \"plus\", \"poate\", \"pot\", \"prea\", \"prima\", \"primul\", \"prin\", \"printr-\",\n",
    "    \"putini\", \"puţin\", \"puţina\", \"puţină\", \"puțin\", \"puțina\", \"puțină\", \"până\",\n",
    "    \"pînă\", \"r\", \"rog\", \"s\", \"sa\", \"sa-mi\", \"sa-ti\", \"sai\", \"sale\", \"sau\",\n",
    "    \"se\", \"si\", \"sint\", \"sintem\", \"spate\", \"spre\", \"sub\", \"sunt\", \"suntem\",\n",
    "    \"sunteţi\", \"sunteți\", \"sus\", \"sută\", \"sînt\", \"sîntem\", \"sînteţi\",\n",
    "    \"sînteți\", \"să\", \"săi\", \"său\", \"t\", \"ta\", \"tale\", \"te\", \"ti\", \"timp\",\n",
    "    \"tine\", \"toata\", \"toate\", \"toată\", \"tocmai\", \"tot\", \"toti\", \"totul\",\n",
    "    \"totusi\", \"totuşi\", \"totuși\", \"toţi\", \"toți\", \"trei\", \"treia\", \"treilea\",\n",
    "    \"tu\", \"tuturor\", \"tăi\", \"tău\", \"u\", \"ul\", \"ului\", \"un\", \"una\", \"unde\",\n",
    "    \"undeva\", \"unei\", \"uneia\", \"unele\", \"uneori\", \"unii\", \"unor\", \"unora\",\n",
    "    \"unu\", \"unui\", \"unuia\", \"unul\", \"v\", \"va\", \"vi\", \"voastre\", \"voastră\",\n",
    "    \"voi\", \"vom\", \"vor\", \"vostru\", \"vouă\", \"voştri\", \"voștri\", \"vreme\", \"vreo\",\n",
    "    \"vreun\", \"vă\", \"x\", \"z\", \"zece\", \"zero\", \"zi\", \"zice\", \"îi\", \"îl\", \"îmi\",\n",
    "    \"împotriva\", \"în\", \"înainte\", \"înaintea\", \"încotro\", \"încât\", \"încît\",\n",
    "    \"între\", \"întrucât\", \"întrucît\", \"îţi\", \"îți\", \"ăla\", \"ălea\", \"ăsta\",\n",
    "    \"ăstea\", \"ăştia\", \"ăștia\", \"şapte\", \"şase\", \"şi\", \"ştiu\", \"ţi\", \"ţie\",\n",
    "    \"șapte\", \"șase\", \"și\", \"știu\", \"ți\", \"ție\"\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T09:55:37.663971Z",
     "start_time": "2025-02-04T09:55:37.662514Z"
    }
   },
   "id": "6f424384dbf01e6b",
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "# # Get all the words from the stop words list and apply the same transformation\n",
    "stop_words = romanian\n",
    "for i in range(len(stop_words)):\n",
    "    stop_words[i] = no_diacritics(stop_words[i], romanian_prefixes)\n",
    "\n",
    "stop_words = list(set(stop_words))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T09:55:39.764167Z",
     "start_time": "2025-02-04T09:55:39.756084Z"
    }
   },
   "id": "a152024aa25a71cb",
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": [
    "def analyze_label_distribution(dataset_folder):\n",
    "    json_files = [f for f in os.listdir(dataset_folder) if f.endswith('.json')]\n",
    "    \n",
    "    dataframes = []\n",
    "    \n",
    "    for filename in json_files:\n",
    "        filepath = os.path.join(dataset_folder, filename)\n",
    "        \n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if 200 <= len(data) <= 1000:\n",
    "            df = pd.DataFrame(data)\n",
    "            dataframes.append(df)\n",
    "        elif len(data) > 1000:\n",
    "            df = pd.DataFrame(data).sample(n=1000, random_state=42)\n",
    "            dataframes.append(df)\n",
    "\n",
    "    if dataframes:\n",
    "        final_dataframe = pd.concat(dataframes, ignore_index=True)\n",
    "        \n",
    "        # Count label distribution\n",
    "        label_counts = final_dataframe['label'].value_counts()\n",
    "        \n",
    "        print(\"Label Distribution:\")\n",
    "        for label, count in label_counts.items():\n",
    "            print(f\"{label}: {count}\")\n",
    "        \n",
    "        return final_dataframe\n",
    "    else:\n",
    "        print(\"No files met the criteria.\")\n",
    "        return None\n",
    "\n",
    "dataset_folder = 'Dataset'\n",
    "data = analyze_label_distribution(dataset_folder)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T09:55:42.680451Z",
     "start_time": "2025-02-04T09:55:42.187740Z"
    }
   },
   "id": "4f053a001f19b23e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Distribution:\n",
      "Banat: 1000\n",
      "Ungheni: 1000\n",
      "Oltenia: 1000\n",
      "Moldova: 1000\n",
      "Serbia: 1000\n",
      "Muntenia: 1000\n",
      "Ucraina: 1000\n",
      "Ardeal: 1000\n",
      "Dobrogea: 965\n",
      "Balti: 948\n",
      "Sangerei: 775\n",
      "Spania: 723\n",
      "Maramures: 656\n",
      "Canada_EN: 641\n",
      "Crisana: 579\n",
      "Orhei: 512\n",
      "Calarasi: 511\n",
      "Criuleni: 509\n",
      "Ialoveni: 504\n",
      "Cahul: 504\n",
      "Soroca: 504\n",
      "Germania: 500\n",
      "UK: 499\n",
      "Bucovina: 428\n",
      "Causeni: 321\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": [
    "def get_features_1(train_text, validation_text, num_features):\n",
    "    tfidf = TfidfVectorizer(max_features=num_features)\n",
    "\n",
    "    train_text_tfidf = tfidf.fit_transform(train_text)\n",
    "\n",
    "    validation_text_tfidf = tfidf.transform(validation_text)\n",
    "\n",
    "    return train_text_tfidf.toarray(), validation_text_tfidf.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T09:55:47.786326Z",
     "start_time": "2025-02-04T09:55:47.776746Z"
    }
   },
   "id": "5f9f92824568c261",
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": [
    "nlp = spacy.load(\"ro_core_news_sm\")\n",
    "\n",
    "def preprocess(text):\n",
    "    text = no_diacritics(text, romanian_prefixes)\n",
    "    text = ' '.join(word for word in text.split() if word.lower() in stop_words)\n",
    "    doc = nlp(text)\n",
    "    text = ' '.join(token.lemma_ for token in doc if not token.is_stop)\n",
    "    \n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T09:55:48.827808Z",
     "start_time": "2025-02-04T09:55:48.601414Z"
    }
   },
   "id": "ee90ddb829549a79",
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": [
    "print(data.head())\n",
    "\n",
    "data = data[data['text'].notna() | data['content'].notna()]\n",
    "\n",
    "data['text'] = data['text'].combine_first(data['content'])\n",
    "\n",
    "texts = data['text']\n",
    "labels = data['label']\n",
    "\n",
    "train_texts, validation_texts, train_labels, validation_labels = train_test_split(texts,labels, test_size=0.2, random_state=42)\n",
    "\n",
    "train_texts.apply(preprocess)\n",
    "validation_texts.apply(preprocess)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T07:17:34.993710Z",
     "start_time": "2025-02-04T07:12:17.664532Z"
    }
   },
   "id": "7abfb665b4c69d9b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  Miercuri și vineri, veniți la un spectacol des...   \n",
      "1  Verginia Cetulean: „Vreau o Moldovă modernă, u...   \n",
      "2  Ești din raionul Soroca și cauți un loc de luc...   \n",
      "3  Vremea la Soroca: prognoza pentru joi, 28 mart...   \n",
      "4  Violeta Boțoc: „Toată copilăria mea am petrecu...   \n",
      "\n",
      "                                             content  \\\n",
      "0  Teatrul „Veniamin Apostol” din Soroca vă invit...   \n",
      "1  Verginia Cetulean, originară din raionul Soroc...   \n",
      "2  Miercuri, 27 noiembrie, la Soroca va fi organi...   \n",
      "3  Potrivit meteorologilor, astăzi vremea va fi f...   \n",
      "4  Toată copilăria mea am petrecut-o într-o avent...   \n",
      "\n",
      "                                      metadata   label text  \n",
      "0  {'original_file': 'Observatorul_0123.html'}  Soroca  NaN  \n",
      "1  {'original_file': 'Observatorul_0089.html'}  Soroca  NaN  \n",
      "2  {'original_file': 'Observatorul_0066.html'}  Soroca  NaN  \n",
      "3  {'original_file': 'Observatorul_0436.html'}  Soroca  NaN  \n",
      "4  {'original_file': 'Observatorul_0318.html'}  Soroca  NaN  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18633    doi an an an an an putin suta an Antre lingă d...\n",
       "115                                an an an an an an an an\n",
       "3314     an an an an an an an an an an an Sunteti an an an\n",
       "12281    doi an an an an an an an dată an an an an ling...\n",
       "13876                                            lingă doi\n",
       "                               ...                        \n",
       "7541     an doi an doi an an an an an doi an an an an a...\n",
       "6586                                  an lingă an an an an\n",
       "16262                        antre an putin an an an an an\n",
       "16721                                   an noastra miin an\n",
       "14001                                an an an an pană aiba\n",
       "Name: text, Length: 3816, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T07:17:35.040498Z",
     "start_time": "2025-02-04T07:17:35.027803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_texts = train_texts[train_texts.str.strip() != '']\n",
    "train_texts = train_texts[train_texts.notna()]\n",
    "validation_texts = validation_texts[validation_texts.str.strip() != '']\n",
    "validation_texts = validation_texts[validation_texts.notna()]\n",
    "\n",
    "train_labels = train_labels[train_texts.index]\n",
    "train_labels = train_labels[train_labels.notna()]\n",
    "validation_labels = validation_labels[validation_texts.index]\n",
    "validation_labels = validation_labels[validation_labels.notna()]\n",
    "\n",
    "final_train_indices = train_texts.index.intersection(train_labels.index)\n",
    "final_val_indices = validation_texts.index.intersection(validation_labels.index)\n",
    "\n",
    "train_texts = train_texts[final_train_indices]\n",
    "train_labels = train_labels[final_train_indices]\n",
    "validation_texts = validation_texts[final_val_indices]\n",
    "validation_labels = validation_labels[final_val_indices]\n",
    "\n",
    "print(\"\\nFinal dataset sizes after cleaning:\")\n",
    "print(f\"Final training samples: {len(train_texts)}\")\n",
    "print(f\"Final validation samples: {len(validation_texts)}\")"
   ],
   "id": "df9e5a7de2df5cb0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final dataset sizes after cleaning:\n",
      "Final training samples: 14441\n",
      "Final validation samples: 3598\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T07:17:35.066285Z",
     "start_time": "2025-02-04T07:17:35.061825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(train_texts[1:20])\n",
    "print(validation_texts[1:20])\n",
    "\n",
    "print(train_labels[1:20])\n",
    "print(validation_labels[1:20])"
   ],
   "id": "425e10b4121aed8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5460     11 barbati şi o femeie, cetăţeni români, au fo...\n",
      "1739     Circulația a fost întreruptă complet miercuri ...\n",
      "14294    Primăria Craiova vine în sprijinul cetățenilor...\n",
      "7775     În primele 10 luni ale anului 2024, poliția di...\n",
      "19048    Ministrul Sănătăţii, Nelu Tătaru, a declarat, ...\n",
      "11056    Orașul Ialoveni se va înfrăți cu alte patru lo...\n",
      "14684    In nicio guvernare nu au existat atatea masuri...\n",
      "2447     Și consilierii raionali de Orhei nu mai vor să...\n",
      "1887             Your email address will not be published.\n",
      "5858     Ministrul delegat al românilor de pretutindeni...\n",
      "16038    Alina Carp, originară din Cahul, este un exemp...\n",
      "10443    Cu temperaturi de până la 36°C, așteptate în a...\n",
      "7568     Trump: \"Voi opri războiul din Ucraina. Voi opr...\n",
      "8056     Este cu ce împușca și cu ce zbura, dar nu are ...\n",
      "15388    Spania este campioana Europei, după ce s-a imp...\n",
      "10780    DOC. Consilierii din Horești, convocați în șed...\n",
      "13662    Pentru cei din departamentul federal de statis...\n",
      "10880    GALERIE FOTO// Cum arată porțiunea asfaltată d...\n",
      "905      Mii de persoane cu vize de rezidență permanent...\n",
      "Name: text, dtype: object\n",
      "115      Asociația Obștească „Eco-Sor” a lansat recent ...\n",
      "3314     Administratorul public al județului Tulcea, Du...\n",
      "12281    Zile la rând, angajații Inspectoratului de Pol...\n",
      "13876    Canada va fi gazda summitului G20 din iunie 20...\n",
      "14117    Un grav accident rutier a avut loc duminică, 1...\n",
      "14123    Ministerul Lucrărilor Publice, Dezvoltării şi ...\n",
      "928      Prințesa se Wales se află în spital, în recupe...\n",
      "13813    Spre deosebire de Statele Unite, in Canada nu ...\n",
      "11656    Anul acesta, orașul german Ulm a organizat a 8...\n",
      "11640    Vineri, 6 octombrie, în cadrul marcării Săptăm...\n",
      "4049     Fostul președinte al administrației județene M...\n",
      "1323     Prefectura Bacău s-a mutat, temporar, la Centr...\n",
      "18855    În cadrul operațiunii Jupiter, la Timișoara au...\n",
      "13169    Cu ocazia zilei de naștere Vă onorăm cu urări ...\n",
      "5887     Rememorăm o zi tragică din istoria recentă a M...\n",
      "13243    Persoanele cu deficiențe de vedere din raion, ...\n",
      "7831     În seara zilei de 13 ianuarie, în satul Pripru...\n",
      "5157     Angenția Națională pentru Siguranța Alimentelo...\n",
      "8805     Asociația Presei Independente face apel la cet...\n",
      "Name: text, dtype: object\n",
      "5460        Spania\n",
      "1739       Moldova\n",
      "14294      Oltenia\n",
      "7775       Ucraina\n",
      "19048        Banat\n",
      "11056     Ialoveni\n",
      "14684      Oltenia\n",
      "2447         Orhei\n",
      "1887       Moldova\n",
      "5858        Spania\n",
      "16038        Cahul\n",
      "10443     Germania\n",
      "7568       Ucraina\n",
      "8056       Ucraina\n",
      "15388      Crisana\n",
      "10780     Ialoveni\n",
      "13662    Canada_EN\n",
      "10880     Ialoveni\n",
      "905             UK\n",
      "Name: label, dtype: object\n",
      "115         Soroca\n",
      "3314      Dobrogea\n",
      "12281     Calarasi\n",
      "13876    Canada_EN\n",
      "14117      Oltenia\n",
      "14123      Oltenia\n",
      "928             UK\n",
      "13813    Canada_EN\n",
      "11656       Serbia\n",
      "11640       Serbia\n",
      "4049        Ardeal\n",
      "1323       Moldova\n",
      "18855        Banat\n",
      "13169     Sangerei\n",
      "5887        Spania\n",
      "13243     Sangerei\n",
      "7831       Ucraina\n",
      "5157      Criuleni\n",
      "8805         Balti\n",
      "Name: label, dtype: object\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "source": [
    "f = open(\"rf-texts-tfidf.txt\", \"w\")\n",
    "\n",
    "for num_features in [500, 1000, 1500, 2000, 2500]:\n",
    "    for num_estimators in [100, 150, 200]:\n",
    "        print(\"=\" * 50)\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "        print(f\"Testing for num_features = {num_features}, num_estimators = {num_estimators}\")\n",
    "        f.write(f\"Testing for num_features = {num_features}, num_estimators = {num_estimators}\\n\")\n",
    "\n",
    "        print(\"Creating text features...\")\n",
    "        f.write(\"Creating text features...\\n\")\n",
    "\n",
    "        train_texts_featurized, validation_texts_featurized = get_features_1(train_texts, validation_texts, num_features)\n",
    "\n",
    "        print(\"Training Random Forest...\")\n",
    "        f.write(\"Training Random Forest...\\n\")\n",
    "        random_forest = RandomForestClassifier(\n",
    "            n_estimators=num_estimators,\n",
    "            random_state=42)\n",
    "        random_forest.fit(train_texts_featurized, train_labels)\n",
    "\n",
    "        print(\"Getting predictions...\")\n",
    "        f.write(\"Getting predictions...\\n\")\n",
    "        predictions = random_forest.predict(validation_texts_featurized)\n",
    "        \n",
    "        print(\"Computing accuracy...\")\n",
    "        f.write(\"Computing accuracy...\")\n",
    "        accuracy = accuracy_score(validation_labels, predictions)\n",
    "\n",
    "        print(\"Computing f1...\")\n",
    "        f.write(\"Computing f1...\\n\")\n",
    "        f1 = f1_score(validation_labels, predictions, average='weighted')\n",
    "\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        f.write(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "        print(f\"F1: {f1}\")\n",
    "        f.write(f\"F1: {f1}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T07:22:59.824167Z",
     "start_time": "2025-02-04T07:17:35.113917Z"
    }
   },
   "id": "3a7cc62f2cd35f2c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Testing for num_features = 500, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.6609227348526959\n",
      "F1: 0.6615604450529123\n",
      "==================================================\n",
      "Testing for num_features = 500, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.669816564758199\n",
      "F1: 0.6707661034391523\n",
      "==================================================\n",
      "Testing for num_features = 500, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.6681489716509171\n",
      "F1: 0.6691854144159909\n",
      "==================================================\n",
      "Testing for num_features = 1000, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.7504168982768205\n",
      "F1: 0.7491143480847187\n",
      "==================================================\n",
      "Testing for num_features = 1000, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.7570872707059477\n",
      "F1: 0.7566212443445087\n",
      "==================================================\n",
      "Testing for num_features = 1000, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.7581989994441356\n",
      "F1: 0.7576431658429635\n",
      "==================================================\n",
      "Testing for num_features = 1500, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.7848804891606448\n",
      "F1: 0.7854936217948544\n",
      "==================================================\n",
      "Testing for num_features = 1500, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.782934963868816\n",
      "F1: 0.7839618241980596\n",
      "==================================================\n",
      "Testing for num_features = 1500, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.7854363535297387\n",
      "F1: 0.7864677454680798\n",
      "==================================================\n",
      "Testing for num_features = 2000, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.7926625903279599\n",
      "F1: 0.7931627095323266\n",
      "==================================================\n",
      "Testing for num_features = 2000, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.7998888271261813\n",
      "F1: 0.8003320686993183\n",
      "==================================================\n",
      "Testing for num_features = 2000, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.7996108949416343\n",
      "F1: 0.7997865639087732\n",
      "==================================================\n",
      "Testing for num_features = 2500, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.8143413007226237\n",
      "F1: 0.8147398577201179\n",
      "==================================================\n",
      "Testing for num_features = 2500, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.8157309616453585\n",
      "F1: 0.8164430200813955\n",
      "==================================================\n",
      "Testing for num_features = 2500, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.8179544191217343\n",
      "F1: 0.8189640801323927\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "source": [
    "def get_features_2(train_text, validation_text, num_features=100, window=3, min_count=1, \n",
    "                   use_skipgram=True):\n",
    "\n",
    "    train_tokens = [text.split() for text in train_text]\n",
    "    validation_tokens = [text.split() for text in validation_text]\n",
    "    \n",
    "    model = Word2Vec(sentences=train_tokens, \n",
    "                     vector_size=num_features, \n",
    "                     window=window, \n",
    "                     min_count=min_count,\n",
    "                     sg=1 if use_skipgram else 0)\n",
    "    \n",
    "    def text_to_features(tokens):\n",
    "        features = []\n",
    "        for text in tokens:\n",
    "            text_vectors = [model.wv[word] for word in text if word in model.wv]\n",
    "            \n",
    "            if not text_vectors:\n",
    "                features.append(np.zeros(num_features))\n",
    "            else:\n",
    "                features.append(np.mean(text_vectors, axis=0))\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    train_features = text_to_features(train_tokens)\n",
    "    validation_features = text_to_features(validation_tokens)\n",
    "    \n",
    "    return train_features, validation_features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T07:22:59.842600Z",
     "start_time": "2025-02-04T07:22:59.838602Z"
    }
   },
   "id": "f8d2886a2139a232",
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "source": [
    "f = open(\"rf-texts-word2vec.txt\", \"w\")\n",
    "\n",
    "for num_features in [500, 1000, 1500, 2000, 2500]:\n",
    "    for num_estimators in [100, 150, 200]:\n",
    "        print(\"=\" * 50)\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "        print(f\"Testing for num_features = {num_features}, num_estimators = {num_estimators}\")\n",
    "        f.write(f\"Testing for num_features = {num_features}, num_estimators = {num_estimators}\\n\")\n",
    "\n",
    "        print(\"Creating text features...\")\n",
    "        f.write(\"Creating text features...\\n\")\n",
    "\n",
    "        train_texts_featurized, validation_texts_featurized = get_features_2(train_texts, validation_texts, num_features)\n",
    "\n",
    "        print(\"Training Random Forest...\")\n",
    "        f.write(\"Training Random Forest...\\n\")\n",
    "        random_forest = RandomForestClassifier(\n",
    "            n_estimators=num_estimators,\n",
    "            random_state=42)\n",
    "        random_forest.fit(train_texts_featurized, train_labels)\n",
    "\n",
    "        print(\"Getting predictions...\")\n",
    "        f.write(\"Getting predictions...\\n\")\n",
    "        predictions = random_forest.predict(validation_texts_featurized)\n",
    "        \n",
    "        print(\"Computing accuracy...\")\n",
    "        f.write(\"Computing accuracy...\\n\")\n",
    "        accuracy = accuracy_score(validation_labels, predictions)\n",
    "\n",
    "        print(\"Computing f1...\")\n",
    "        f.write(\"Computing f1...\\n\")\n",
    "        f1 = f1_score(validation_labels, predictions, average='weighted')\n",
    "\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        f.write(f\"Accuracy: {accuracy}\\n\")\n",
    "\n",
    "        print(f\"F1: {f1}\")\n",
    "        f.write(f\"F1: {f1}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-04T08:37:58.644435Z",
     "start_time": "2025-02-04T07:22:59.895947Z"
    }
   },
   "id": "8f76e6bf35123dc6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Testing for num_features = 500, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.5494719288493608\n",
      "F1: 0.5377898041624213\n",
      "==================================================\n",
      "Testing for num_features = 500, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.556420233463035\n",
      "F1: 0.5454083075004874\n",
      "==================================================\n",
      "Testing for num_features = 500, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.5630906058921623\n",
      "F1: 0.5510627233058135\n",
      "==================================================\n",
      "Testing for num_features = 1000, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.5561423012784881\n",
      "F1: 0.5457892313172258\n",
      "==================================================\n",
      "Testing for num_features = 1000, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.5614230127848805\n",
      "F1: 0.5498444658758107\n",
      "==================================================\n",
      "Testing for num_features = 1000, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.566425792106726\n",
      "F1: 0.5539198652657848\n",
      "==================================================\n",
      "Testing for num_features = 1500, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.5458588104502501\n",
      "F1: 0.5356288564550924\n",
      "==================================================\n",
      "Testing for num_features = 1500, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.5603112840466926\n",
      "F1: 0.5496024858763758\n",
      "==================================================\n",
      "Testing for num_features = 1500, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.5630906058921623\n",
      "F1: 0.5510719382198374\n",
      "==================================================\n",
      "Testing for num_features = 2000, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.5578098943857699\n",
      "F1: 0.5488163003606289\n",
      "==================================================\n",
      "Testing for num_features = 2000, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.5611450806003335\n",
      "F1: 0.5506817595733061\n",
      "==================================================\n",
      "Testing for num_features = 2000, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.5633685380767093\n",
      "F1: 0.5530002945809287\n",
      "==================================================\n",
      "Testing for num_features = 2500, num_estimators = 100\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.5605892162312396\n",
      "F1: 0.5504515356271705\n",
      "==================================================\n",
      "Testing for num_features = 2500, num_estimators = 150\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.5639244024458032\n",
      "F1: 0.5528869447728807\n",
      "==================================================\n",
      "Testing for num_features = 2500, num_estimators = 200\n",
      "Creating text features...\n",
      "Training Random Forest...\n",
      "Getting predictions...\n",
      "Computing accuracy...\n",
      "Computing f1...\n",
      "Accuracy: 0.5694830461367426\n",
      "F1: 0.559309476002964\n"
     ]
    }
   ],
   "execution_count": 30
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
