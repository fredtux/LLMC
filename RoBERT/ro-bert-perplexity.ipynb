{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10654083,"sourceType":"datasetVersion","datasetId":6597502}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers torch datasets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T09:21:53.898260Z","iopub.execute_input":"2025-02-04T09:21:53.898551Z","iopub.status.idle":"2025-02-04T09:21:58.245382Z","shell.execute_reply.started":"2025-02-04T09:21:53.898528Z","shell.execute_reply":"2025-02-04T09:21:58.244532Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport math","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T09:22:37.969894Z","iopub.execute_input":"2025-02-04T09:22:37.970230Z","iopub.status.idle":"2025-02-04T09:22:37.974284Z","shell.execute_reply.started":"2025-02-04T09:22:37.970204Z","shell.execute_reply":"2025-02-04T09:22:37.973387Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_name = \"readerbench/RoBERT-base\"  # Romanian RoBERTa\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T09:22:00.496466Z","iopub.execute_input":"2025-02-04T09:22:00.496799Z","iopub.status.idle":"2025-02-04T09:22:25.210488Z","shell.execute_reply.started":"2025-02-04T09:22:00.496774Z","shell.execute_reply":"2025-02-04T09:22:25.209393Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T09:22:40.320422Z","iopub.execute_input":"2025-02-04T09:22:40.320745Z","iopub.status.idle":"2025-02-04T09:22:41.339526Z","shell.execute_reply.started":"2025-02-04T09:22:40.320717Z","shell.execute_reply":"2025-02-04T09:22:41.338803Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_metrics(model, tokenizer, text, verbose=False):\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    logits = outputs.logits\n    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n    neg_log_likelihood = -torch.mean(torch.log(probabilities.max(dim=-1).values + 1e-9))\n\n    # Only print debugging info if verbose=True\n    if verbose:\n        print(f\"Text: {text[:50]}... | NLL: {neg_log_likelihood:.4f}\")\n\n    return neg_log_likelihood.item(), neg_log_likelihood.item()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T09:22:44.381805Z","iopub.execute_input":"2025-02-04T09:22:44.382105Z","iopub.status.idle":"2025-02-04T09:22:44.387632Z","shell.execute_reply.started":"2025-02-04T09:22:44.382082Z","shell.execute_reply":"2025-02-04T09:22:44.386728Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T09:22:46.668052Z","iopub.execute_input":"2025-02-04T09:22:46.668382Z","iopub.status.idle":"2025-02-04T09:22:46.671856Z","shell.execute_reply.started":"2025-02-04T09:22:46.668359Z","shell.execute_reply":"2025-02-04T09:22:46.671085Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# regions = [\n#     'Ardeal',\n#     'Banat',\n#     'Bucovina',\n#     'Canada_EN',\n#     'Canada_Quebec',\n#     'Crisana',\n#     'Dobrogea',\n#     'Germania',\n#     'Italia',\n#     'Maramures',\n#     'Moldova',\n#     'Muntenia',\n#     'Oltenia',\n#     'Serbia',\n#     'Spania',\n#     'Ucraina',\n#     'UK'\n# ]\nregions = [\n    'Balti',\n    'Cahul',\n    'Calarasi',\n    'Causeni',\n    'Comrat',\n    'Criuleni',\n    'Hincesti',\n    'Ialoveni',\n    'Orhei',\n    'Sangerei',\n    'Soroca',\n    'Ungheni',\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T09:24:54.316413Z","iopub.execute_input":"2025-02-04T09:24:54.316749Z","iopub.status.idle":"2025-02-04T09:24:54.320843Z","shell.execute_reply.started":"2025-02-04T09:24:54.316721Z","shell.execute_reply":"2025-02-04T09:24:54.319858Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nfrom tqdm import tqdm\nimport numpy as np\n\nwith open(\"/kaggle/input/dataset-tari/dataset/Italia.json\") as f:\n    italia = json.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T09:24:59.002071Z","iopub.execute_input":"2025-02-04T09:24:59.002391Z","iopub.status.idle":"2025-02-04T09:24:59.007437Z","shell.execute_reply.started":"2025-02-04T09:24:59.002364Z","shell.execute_reply":"2025-02-04T09:24:59.006563Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport numpy as np\nfrom tqdm import tqdm\n\nresults_regions = {}\n\nfor region in regions:\n    results_regions[region] = []\n    results_content = {\"perplexity\": [], \"neg_log_likelihood\": []}\n    results_titles = {\"perplexity\": [], \"neg_log_likelihood\": []}\n\n    with open(f\"/kaggle/input/dataset-tari/dataset/Rep_Moldova/{region}.json\") as f:\n        region_json = json.load(f)\n\n    for row in tqdm(region_json, disable=True):  # Disable progress bar output\n        row_cnt_result = compute_metrics(model, tokenizer, row['content'] if 'content' in row else row['text'], verbose=False)\n        results_content['perplexity'].append(row_cnt_result[0])\n        results_content['neg_log_likelihood'].append(row_cnt_result[1])\n        \n        row_title_result = compute_metrics(model, tokenizer, row['title'], verbose=False)\n        results_titles['perplexity'].append(row_title_result[0])\n        results_titles['neg_log_likelihood'].append(row_title_result[1])\n\n    perp_content_mean = np.array(results_content['neg_log_likelihood']).mean()\n    perp_titles_mean = np.array(results_titles['neg_log_likelihood']).mean()\n\n    results_regions[region].append({\n        'content': results_content,\n        'titles': results_titles,\n        'perp_mean_content': perp_content_mean,\n        'perp_mean_titles': perp_titles_mean\n    })\n\nwith open(\"/kaggle/working/results_regions.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(results_regions, f, indent=4)\n\nprint(\"\\n‚úÖ Results successfully saved to /kaggle/working/results_regions.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T09:39:26.915134Z","iopub.execute_input":"2025-02-04T09:39:26.915460Z","iopub.status.idle":"2025-02-04T09:41:47.032712Z","shell.execute_reply.started":"2025-02-04T09:39:26.915439Z","shell.execute_reply":"2025-02-04T09:41:47.031798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport numpy as np\n\n# Load results\nwith open(\"/kaggle/working/results_regions.json\", \"r\", encoding=\"utf-8\") as f:\n    results_regions = json.load(f)\n\n# Store all perplexities for overall statistics\nall_perplexities_content = []\nall_perplexities_titles = []\n\nprint(\"\\nüìä **Perplexity Statistics Per Region:**\")\nfor region, data in results_regions.items():\n    region_perplexities_content = [p for entry in data for p in entry[\"content\"][\"perplexity\"]]\n    region_perplexities_titles = [p for entry in data for p in entry[\"titles\"][\"perplexity\"]]\n\n    if not region_perplexities_content or not region_perplexities_titles:\n        print(f\"‚ö†Ô∏è No data available for {region}. Skipping...\")\n        continue  # Avoid crashing if a region has no perplexities\n\n    # Compute mean, min, and max correctly from all values\n    mean_content = np.mean(region_perplexities_content)\n    min_content = np.min(region_perplexities_content)\n    max_content = np.max(region_perplexities_content)\n\n    mean_titles = np.mean(region_perplexities_titles)\n    min_titles = np.min(region_perplexities_titles)\n    max_titles = np.max(region_perplexities_titles)\n\n    # Print per-region statistics\n    print(f\"üìç {region}:\")\n    print(f\"   - **Content**: Mean = {mean_content:.4f}, Min = {min_content:.4f}, Max = {max_content:.4f}\")\n    print(f\"   - **Titles**:  Mean = {mean_titles:.4f}, Min = {min_titles:.4f}, Max = {max_titles:.4f}\\n\")\n\n    # Collect for overall dataset statistics\n    all_perplexities_content.extend(region_perplexities_content)\n    all_perplexities_titles.extend(region_perplexities_titles)\n\n# Compute overall dataset statistics\nif all_perplexities_content and all_perplexities_titles:\n    dataset_mean_content = np.mean(all_perplexities_content)\n    dataset_min_content = np.min(all_perplexities_content)\n    dataset_max_content = np.max(all_perplexities_content)\n\n    dataset_mean_titles = np.mean(all_perplexities_titles)\n    dataset_min_titles = np.min(all_perplexities_titles)\n    dataset_max_titles = np.max(all_perplexities_titles)\n\n    # Print overall dataset statistics\n    print(\"\\nüìä **Overall Dataset Perplexity Statistics:**\")\n    print(f\"   - **Content**: Mean = {dataset_mean_content:.4f}, Min = {dataset_min_content:.4f}, Max = {dataset_max_content:.4f}\")\n    print(f\"   - **Titles**:  Mean = {dataset_mean_titles:.4f}, Min = {dataset_min_titles:.4f}, Max = {dataset_max_titles:.4f}\")\nelse:\n    print(\"‚ö†Ô∏è No perplexity data found in the dataset.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T09:50:48.039783Z","iopub.execute_input":"2025-02-04T09:50:48.040146Z","iopub.status.idle":"2025-02-04T09:50:48.073123Z","shell.execute_reply.started":"2025-02-04T09:50:48.040121Z","shell.execute_reply":"2025-02-04T09:50:48.072423Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nprint(\"File exists:\", os.path.exists(\"/kaggle/working/results_regions.json\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T09:48:13.346230Z","iopub.execute_input":"2025-02-04T09:48:13.346568Z","iopub.status.idle":"2025-02-04T09:48:13.351791Z","shell.execute_reply.started":"2025-02-04T09:48:13.346543Z","shell.execute_reply":"2025-02-04T09:48:13.350892Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport numpy as np\nimport nltk\nfrom nltk.tokenize import sent_tokenize\nimport os\n\nnltk.download('punkt')\n\ndataset_path = \"/kaggle/input/dataset-tari/dataset/Rep_Moldova\"\n\nregion_files = [f for f in os.listdir(dataset_path) if f.endswith(\".json\")]\n\nall_perplexities_sentences = []\n\nprint(\"\\nüìä **Perplexity Statistics Per Region (Sentence-Level):**\")\nfor region_file in region_files:\n    region_name = region_file.replace(\".json\", \"\")  # Extract region name\n    region_perplexities_sentences = []\n\n    # Load JSON data\n    with open(os.path.join(dataset_path, region_file), \"r\", encoding=\"utf-8\") as f:\n        region_data = json.load(f)\n\n    for entry in region_data:\n        content_text = entry.get(\"content\", \"\")\n        title_text = entry.get(\"title\", \"\")\n\n        combined_text = f\"{content_text} {title_text}\".strip()  # Combine content and title text\n\n        if not combined_text:\n            print(f\"‚ö†Ô∏è No text data found for {region_name}. Skipping...\")\n            continue  # Skip empty text regions\n\n        # Split text into sentences\n        sentences = sent_tokenize(combined_text)\n\n        for sentence in sentences:\n            if sentence.strip():  # Ignore empty sentences\n                # Compute perplexity for each sentence\n                sentence_result = compute_metrics(model, tokenizer, sentence)\n                region_perplexities_sentences.append(sentence_result[0])  # Perplexity value\n\n    if not region_perplexities_sentences:\n        print(f\"‚ö†Ô∏è No valid sentence-level data for {region_name}. Skipping...\")\n        continue  # Avoid crashing if no sentences exist\n\n    # Compute mean, min, and max correctly from all values\n    mean_sentences = np.mean(region_perplexities_sentences)\n    min_sentences = np.min(region_perplexities_sentences)\n    max_sentences = np.max(region_perplexities_sentences)\n\n    # Print per-region statistics\n    print(f\"üìç {region_name}:\")\n    print(f\"   - **Sentences**: Mean = {mean_sentences:.4f}, Min = {min_sentences:.4f}, Max = {max_sentences:.4f}\\n\")\n\n    # Collect for overall dataset statistics\n    all_perplexities_sentences.extend(region_perplexities_sentences)\n\n# Compute overall dataset statistics\nif all_perplexities_sentences:\n    dataset_mean_sentences = np.mean(all_perplexities_sentences)\n    dataset_min_sentences = np.min(all_perplexities_sentences)\n    dataset_max_sentences = np.max(all_perplexities_sentences)\n\n    print(\"\\nüìä **Overall Dataset Perplexity Statistics (Sentence-Level):**\")\n    print(f\"   - **Sentences**: Mean = {dataset_mean_sentences:.4f}, Min = {dataset_min_sentences:.4f}, Max = {dataset_max_sentences:.4f}\")\nelse:\n    print(\"‚ö†Ô∏è No perplexity data found in the dataset.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T10:25:58.944504Z","iopub.execute_input":"2025-02-04T10:25:58.944800Z","iopub.status.idle":"2025-02-04T10:36:25.052775Z","shell.execute_reply.started":"2025-02-04T10:25:58.944779Z","shell.execute_reply":"2025-02-04T10:36:25.051528Z"}},"outputs":[],"execution_count":null}]}